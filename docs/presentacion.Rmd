---
title: "Introducci√≥n a LightGBM con R" 
author: "Carlos A. Torres Cubilla"
output:
  slidy_presentation: 
    number_sections: false
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: false
    includes:
      in_header: styles_and_copy_button.html
---

```{r setup, include=FALSE}
library(htmltools)
library(DiagrammeR)
library(DT)
knitr::opts_chunk$set(
  echo = TRUE, 
  message = FALSE, 
  warning = FALSE,
  fig.align='center'
)
```

# ¬øQu√© es LightGBM?

LightGBM es un framework de *gradient boosting* que utiliza algoritmos de aprendizaje basados en √°rboles para crear modelos predictivos de manera eficiente y r√°pida. 

## ¬øQue significa LightGBM?

- üî¶ **Light** = ligero, r√°pido, optimizado
  - Se refiere a que es una versi√≥n optimizada del algoritmo de Gradient Boosting.
  - Est√° dise√±ado para consumir menos memoria y funcionar m√°s r√°pido que alternativas como XGBoost o Random Forests.
- üìà **GBM** = *Gradient Boosting Machine*
  - Un tipo de t√©cnica de ensemble learning que genera muchos modelos d√©biles (generalmente √°rboles de decisi√≥n) de forma secuencial, donde cada modelo aprende de los errores de sus predecesores para crear un modelo fuerte .
  - Se basa en usar el gradiente del error para mejorar paso a paso.

<div style="text-align: center;">
  <img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiN2KPoea9rFZo4nb0SZKrBrEUjNv-xaqB7gF6Htl5lY5AtOmKH1yFalD9Y6XHNNgtUYqsJCPUr-7a4MJIvdcubXogxerrskVqKfQGhKSpUyrnroLhEi6P5vMXqYE22J3_dnLRuWiBv5Nw/s0/Random+Forest+03.gif" style="display: block; margin-left: auto; margin-right: auto;" width="75%"/>
</div>

<div class="gradient-boosting" style="background-color: #f9f9f9; border-left: 5px solid #007ACC; padding: 10px; margin-top: 0px;">
  LightGBM mejora los errores paso a paso combinando √°rboles de decisi√≥n peque√±os.
</div>

## ¬øQui√©n cre√≥ LightGBM?

<div style="display: flex; align-items: center; gap: 1rem;">
  <img src="https://upload.wikimedia.org/wikipedia/commons/9/96/Microsoft_logo_%282012%29.svg" 
       alt="Microsoft Logo" 
       style="height: 50px; margin-bottom: 0;" />

<div>
  <p style="margin: 0;">
    Desarrollado por <strong>Microsoft Research Asia</strong>  
    como parte del toolkit <strong>DMTK</strong> (*Distributed Machine Learning Toolkit*).
  </p>
  <p style="margin: 0;">
    Lanzado en <strong>2016</strong> como proyecto <em>open source</em>.
  </p>
</div>
</div>

> <em>"Creamos LightGBM para resolver problemas del mundo real a gran escala con eficiencia."</em>  
> ‚Äî <strong>Microsoft DMTK Team</strong>

# ¬øC√≥mo funciona el Gradient Boosting?

Podemos resumir el funcionamiento del *Gradient Boosting* en los siguientes 5 pasos: 

```{r boosting_paso_a_paso, echo=FALSE}
step_style <- "
  display: flex;
  flex-direction: column;
  align-items: center;
  width: 160px;
  border: 1px solid #ccc;
  border-radius: 12px;
  padding: 12px;
  box-shadow: 2px 2px 8px rgba(0,0,0,0.1);
  transition: transform 0.3s;
"

steps <- tagList(
  tags$div(
    style = step_style,
    tags$span(style = "font-size: 30px;", "üìÖ"),
    tags$strong("1. Predicci√≥n Inicial"),
    tags$p("Promedio de la variable objetivo o log-odds si es clasificaci√≥n.")
  ),
  tags$div(
    style = step_style,
    tags$span(style = "font-size: 30px;", "üìà"),
    tags$strong("2. C√°lculo de Error"),
    tags$p("Se calcula la diferencia entre la predicci√≥n y el valor real.")
  ),
  tags$div(
    style = step_style,
    tags$span(style = "font-size: 30px;", "üå≥"),
    tags$strong("3. Nuevo √°rbol"),
    tags$p("Se entrena un √°rbol sobre los errores residuales (gradientes).")
  ),
  tags$div(
    style = step_style,
    tags$span(style = "font-size: 30px;", "‚öñÔ∏è"),
    tags$strong("4. Ponderar"),
    tags$p("Se multiplica por el learning rate para suavizar el ajuste.")
  ),
  tags$div(
    style = step_style,
    tags$span(style = "font-size: 30px;", "‚ûï"),
    tags$strong("5. Sumar"),
    tags$p("Se agrega el √°rbol al modelo existente.")
  )
)

tags$div(
  style = "display: flex; justify-content: center; gap: 20px; flex-wrap: wrap; margin-top: 20px;",
  steps
)
```


üîÑ El proceso se repite muchas veces, agregando un √°rbol nuevo en cada paso, hasta que se alcanza el n√∫mero m√°ximo de rondas (nrounds) o se activa un criterio de parada temprana (early_stopping_rounds) si el rendimiento en el conjunto de validaci√≥n deja de mejorar.

<center>
  <img src="https://media.geeksforgeeks.org/wp-content/uploads/20250519125344578128/python.webp" width="800" />
</center>

Estos pasos se pueden expresar de manera matem√°tica de manera facil

### Paso 1: Predicci√≥n Inicial

El modelo comienza con una predicci√≥n constante para todos los datos.

- En regresi√≥n, suele ser: \(F_0(x) = \bar{y} \)

- En clasificaci√≥n binaria, suele ser: \( F_0(x) = \log\left(\frac{p}{1 - p}\right), \quad p = \frac{\text{# de clase 1}}{n} \)

### Paso 2: Calcular el Error o Gradiente

Para cada observaci√≥n, se calcula cu√°nto se est√° equivocando el modelo actual:

$$ g_i^{1} = y_i - F_{0}(x_i) $$

Este paso corresponde al c√°lculo del gradiente de la funci√≥n de p√©rdida con respecto a la predicci√≥n actual.

### Paso 3: Entrenar un nuevo modelo sobre ese error

Se entrena un modelo d√©bil que aprenda a corregir esos errores: \( h_1(x) \approx g_i^{1} \)

### Paso 4: Escalar la correcci√≥n con un Learning Rate
Se ajusta la magnitud de la correcci√≥n aplicando un factor de aprendizaje \( \eta \) (por ejemplo, 0.1):

$$ \text{correcci√≥n} = \eta \cdot h_1(x) $$

Esto suaviza el aprendizaje y previene el sobreajuste.

### Paso 5: Actualizar el modelo

Se suma la correcci√≥n al modelo acumulado anterior:

$$ F_m(x) = F_0(x) + \eta \cdot h_1(x) $$

Este proceso se repite durante varias iteraciones, agregando un nuevo modelo d√©bil \( h(x) \) cada vez, hasta alcanzar un n√∫mero m√°ximo de rondas (*nrounds*) o un criterio de parada temprana (*early_stopping_rounds*)

### Resultado final

La predicci√≥n final del modelo se puede representar mediante la siguiente funci√≥n:

$$ F_m(x) = F_0(x) + \sum_{m=1}^M \eta \cdot h_m(x) $$

Donde: 

- \( F_m(x) \): predicci√≥n final,
- \( \eta \): learning rate,
- \( h_m(x) \): modelo d√©bil,
- \( M \): n√∫mero total de iteraciones.

En la siguiente imagen se representa visualmente el proceso de mejora paso a paso de un modelo basado en *Gradien Boosting*. 

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.align='center'}
library(lightgbm)
library(ggplot2)
library(dplyr)

# Datos simulados
set.seed(123)
n <- 300
x <- runif(n, 0, 10)
y <- sin(x) + rnorm(n, 0, 0.3)
data <- data.frame(x = x, y = y)

# Dataset para LightGBM
X_mat <- matrix(x, ncol = 1)
dtrain <- lgb.Dataset(data = X_mat, label = y)

# Modelo completo con 10 iteraciones
params <- list(
  objective = "regression",
  learning_rate = 0.3,
  num_leaves = 4,
  max_depth = 2,
  verbosity = -1
)
model <- lgb.train(params, dtrain, nrounds = 12)

# Predicciones paso a paso
predictions <- lapply(1:12, function(i) {
  data.frame(
    x = x,
    y_pred = predict(model, X_mat, num_iteration = i),
    paso = paste("Paso", i)
  )
}) |> bind_rows()

# Convertir a factor con niveles ordenados
predictions$paso <- factor(predictions$paso, levels = paste("Paso", 1:12))

# Visualizaci√≥n
ggplot(predictions, aes(x = x, y = y_pred)) +
  geom_line(color = "blue", alpha = 0.7) +
  geom_point(data = data, aes(x = x, y = y), alpha = 0.2) +
  facet_wrap(~ paso, ncol = 4) +
  theme_minimal(base_size = 12) +
  labs(title = "Gradient Boosting: Mejora paso a paso", x = "x", y = "Predicci√≥n") +
  theme(plot.title = element_text(hjust = 0.5)) 
```

Cada iteraci√≥n mejora el modelo poco a poco, corrigiendo errores anteriores.

# Ventajas y desventajas

LightGBM se destaca no solo por su rendimiento predictivo, sino tambi√©n por su eficiencia computacional. Su adopci√≥n generalizada en aplicaciones reales de machine learning se puede explicar mediante cinco ventajas clave.

## Ventajas

```{r ventajas, echo=FALSE}
ventajas_style <- "
  display: flex; 
  flex-direction: column; 
  align-items: center; 
  width: 120px; 
  transition: transform 0.3s;
"

tags$div(
  style = "display: flex; justify-content: center; gap: 40px; margin-top: 20px;",

  tags$div(
    style = ventajas_style,
    onmouseover = sprintf("this.style.transform='scale(1.1)'"),
    onmouseout = sprintf("this.style.transform='scale(1)'"),
    tags$span(style = "font-size: 40px;", "‚ö°"),
    tags$p("Velocidad")
  ),

  tags$div(
    style = ventajas_style,
    onmouseover = sprintf("this.style.transform='scale(1.1)'"),
    onmouseout = sprintf("this.style.transform='scale(1)'"),
    tags$span(style = "font-size: 40px;", "üíæ"),
    tags$p("Memoria")
  ),

  tags$div(
    style = ventajas_style,
    onmouseover = sprintf("this.style.transform='scale(1.1)'"),
    onmouseout = sprintf("this.style.transform='scale(1)'"),
    tags$span(style = "font-size: 40px;", "üéØ"),
    tags$p("Precisi√≥n")
  ),

  tags$div(
    style = "display: flex; flex-direction: column; align-items: center; width: 120px; transition: transform 0.3s;",
    onmouseover = sprintf("this.style.transform='scale(1.1)'"),
    onmouseout = sprintf("this.style.transform='scale(1)'"),
    tags$span(style = "font-size: 40px;", "üñ•Ô∏è"),
    tags$p("Paralelismo")
  ),

  tags$div(
    style = ventajas_style,
    onmouseover = sprintf("this.style.transform='scale(1.1)'"),
    onmouseout = sprintf("this.style.transform='scale(1)'"),
    tags$span(style = "font-size: 40px;", "üìä"),
    tags$p("Escalabilidad")
  )
)
```

<div style="margin: 0 auto; width: fit-content;">
| Ventaja       | Descripci√≥n                                                                 |
|---------------|------------------------------------------------------------------------------|
| Velocidad   | Entrena modelos r√°pidamente, lo que lo hace ideal para proyectos con grandes vol√∫menes de datos |
| Memoria     | Optimiza el consumo de memoria, permitiendo trabajar con datasets grandes sin requerir tanta RAM         |
| Precisi√≥n   | Proporciona resultados precisos y competitivos en tareas de predicci√≥n                    |
|  Paralelismo | Aprovecha m√∫ltiples n√∫cleos, cl√∫steres y aceleraci√≥n por GPU para entrenamientos m√°s r√°pidos            |
| Escalabilidad | Maneja grandes vol√∫menes de datos sin p√©rdida de rendimiento       |
</div>

## Desventajas

```{r desventajas, echo=FALSE}
desventajas_style <- "
  display: flex; 
  flex-direction: column; 
  align-items: center; 
  width: 130px; 
  transition: transform 0.3s;
"

tags$div(
  style = "display: flex; justify-content: center; flex-wrap: wrap; gap: 40px; margin-top: 20px;",

  tags$div(
    style = desventajas_style,
    onmouseover = "this.style.transform='scale(1.1)'",
    onmouseout = "this.style.transform='scale(1)'",
    tags$span(style = "font-size: 40px;", "‚ö†Ô∏è"),
    tags$p("Overfitting")
  ),

  tags$div(
    style = desventajas_style,
    onmouseover = "this.style.transform='scale(1.1)'",
    onmouseout = "this.style.transform='scale(1)'",
    tags$span(style = "font-size: 40px;", "üîç"),
    tags$p("Dif√≠cil de interpretar")
  ),

  tags$div(
    style = desventajas_style,
    onmouseover = "this.style.transform='scale(1.1)'",
    onmouseout = "this.style.transform='scale(1)'",
    tags$span(style = "font-size: 40px;", "üìâ"),
    tags$p("Problemas en datasets peque√±os")
  ),

  tags$div(
    style = desventajas_style,
    onmouseover = "this.style.transform='scale(1.1)'",
    onmouseout = "this.style.transform='scale(1)'",
    tags$span(style = "font-size: 40px;", "üß†"),
    tags$p("Curva de aprendizaje")
  ),

  tags$div(
    style = desventajas_style,
    onmouseover = "this.style.transform='scale(1.1)'",
    onmouseout = "this.style.transform='scale(1)'",
    tags$span(style = "font-size: 40px;", "üîå"),
    tags$p("Soporte parcial en entornos")
  )
)
```

<div style="margin: 0 auto; width: fit-content;">
| ‚ö†Ô∏è Desventaja                | üìù Descripci√≥n                                                                 |
|-----------------------------|--------------------------------------------------------------------------------|
| Overfitting                 | Al ser muy poderoso, puede memorizar ruido si no se ajusta correctamente.      |
| Dif√≠cil de interpretar      | Es una "caja negra" frente a modelos m√°s simples como regresi√≥n lineal.        |
| Problemas en datasets peque√±os | Modelos simples pueden generalizar mejor en conjuntos de datos chicos.          |
| Curva de aprendizaje        | Muchos hiperpar√°metros que requieren experiencia para ajustar bien.            |
| Soporte parcial en entornos | APIs disponibles, pero algunas integraciones pueden ser limitadas o inestables. |
</div>



# ¬øPara qu√© utilizar LightGBM?

LightGBM es  altamente vers√°til que permite resolver una amplia variedad de problemas, desde tareas tradicionales como clasificaci√≥n y regresi√≥n, hasta aplicaciones m√°s complejas como ranking, detecci√≥n de fraude y sistemas de recomendaci√≥n personalizados. Esto lo convierte en una herramienta poderosa y escalable para proyectos de aprendizaje autom√°tico en entornos reales.

```{r uso, echo=FALSE}
browsable(
  tags$div(
    style = "
      display: flex;
      justify-content: center;
      align-items: center;
      background: transparent;
    ",
    tags$style(HTML("
      /* Reset total de margenes/paddings en Slidy */
      body, html, .slide, .content {
        margin: 0 !important;
        padding: 0 !important;
        height: 0% !important;
      }

      /* El SVG generado por DiagrammeR */
      svg {
        display: block;
        margin: 0;
        padding: 0;
        height: auto;
        width: 0;
        max-height: 30vh;
      }

      .grViz {
        display: flex;
        justify-content: center;
        align-items: center;
        padding: 0 !important;
        margin: 0 !important;
        height: auto;
        width: 0;
        max-height: 30vh;
      }
      }
    ")),
    grViz("
      digraph lightgbm {
        graph [rankdir = TB, margin=0, nodesep=0.3, ranksep=0.3]
        node [shape = box, style = filled, fillcolor = LightGray, fontsize = 12, margin=0.05]

        Usos [label = '¬øPara qu√© se usa?', shape=box, fillcolor=lightblue]
        Clasificacion [label = 'Clasificaci√≥n']
        Regresion [label = 'Regresi√≥n']
        Ranking [label = 'Ranking']
        DeteccionFraude [label = 'Detecci√≥n de\\nfraude']
        Recomendacion [label = 'Sistemas de\\nrecomendaci√≥n']

        Usos -> Clasificacion
        Usos -> Regresion
        Usos -> Ranking
        Usos -> DeteccionFraude
        Usos -> Recomendacion
      }
    ")
  )
)
```

Much√≠simas empresas en todo el mundo utilizan LightGBM debido a su velocidad y precisi√≥n para resolver distintos problemas de machine learning. Algunos ejemplos de compa√±√≠as l√≠deres que han implementado LightGBM en sus soluciones para estos casos de uso:

```{r empresas, echo=FALSE}
html <- tags$div(
  # Estilo del contenedor general
  style = "display: flex; justify-content: center; flex-wrap: wrap; gap: 40px; margin-top: 20px;",
  
  # Estilo para flip cards
  tags$style(HTML("
    .flip-card {
      background-color: transparent;
      width: 175px;
      height: 250px;
      perspective: 1000px;
    }

    .flip-card-inner {
      position: relative;
      width: 100%;
      height: 100%;
      transform-style: preserve-3d;
      transition: transform 0.6s ease;
      transform-origin: center center;
    }

    .flip-card:hover .flip-card-inner {
      transform: rotateY(180deg);
    }

    .flip-card-front, .flip-card-back {
      position: absolute;
      width: 100%;
      height: 100%;
      backface-visibility: hidden;
      border-radius: 10px;
      box-shadow: 0 4px 8px rgba(0,0,0,0.1);
      display: flex;
      flex-direction: column;
      align-items: center;
      justify-content: center;
      padding: 10px;
      box-sizing: border-box;
      text-align: center;
      overflow: hidden;
    }

    .flip-card-front {
      background: white;
    }

    .flip-card-back {
      background-color: #f0f0f0;
      transform: rotateY(180deg);
      font-size: 0.75em;
    }

    .flip-card img {
      width: 90px;
      height: auto;
      margin-bottom: 10px;
    }
    
  ")),

  # Contenido de las tarjetas
  lapply(list(
    list(nombre = "Microsoft", uso = "Clasificaci√≥n", logo = "https://upload.wikimedia.org/wikipedia/commons/9/96/Microsoft_logo_%282012%29.svg", detalle = "Categorizar correos y detectar spam"),
    list(nombre = "Uber", uso = "Regresi√≥n", logo = "https://upload.wikimedia.org/wikipedia/commons/5/58/Uber_logo_2018.svg", detalle = "Estimar demanda y tiempos de llegada"),
    list(nombre = "LinkedIn", uso = "Ranking", logo = "https://upload.wikimedia.org/wikipedia/commons/thumb/a/aa/LinkedIn_2021.svg/2560px-LinkedIn_2021.svg.png", detalle = "Ordenar resultados de b√∫squeda y recomendaciones de empleo"),
    list(nombre = "PayPal", uso = "Detecci√≥n de fraude", logo = "https://upload.wikimedia.org/wikipedia/commons/b/b5/PayPal.svg", detalle = "Detectar transacciones sospechosas en tiempo real"),
    list(nombre = "Netflix", uso = "Sistemas de recomendaci√≥n", logo = "https://upload.wikimedia.org/wikipedia/commons/0/08/Netflix_2015_logo.svg", detalle = "Personalizar sugerencias de contenido")
  ), function(e) {
    tags$div(class = "flip-card",
      tags$div(class = "flip-card-inner",
        tags$div(class = "flip-card-front",
          tags$img(src = e$logo),
          tags$p(e$uso, style = "color: gray; font-size: 0.85em;")
        ),
        tags$div(class = "flip-card-back",
          tags$strong(e$nombre),
          tags$p(e$detalle)
        )
      )
    )
  })
)

browsable(html)
```

# Par√°metros de LightGBM (I)

Cuando entrenamos un modelo con lightgbm, necesitamos decirle c√≥mo aprender. Los par√°metros del modelo le indican a LightGBM como y que tanto aprender. Para una informaci√≥n m√°s completa de los par√°metros de LightGBM visitar la [documentaci√≥n oficial en R](https://lightgbm.readthedocs.io/en/latest/R/index.html).

## ‚öôÔ∏è Par√°metros generales

### 1. `objective`

Es el par√°metro m√°s importante de todos. Le dice a LightGBM qu√© tipo de problema est√°s resolviendo para que use la funci√≥n de p√©rdida adecuada durante el entrenamiento. 

#### üéì Tipos de `objective` m√°s comunes

| Tipo de problema                            | Valor de `objective` | ¬øQu√© hace internamente? |
| ------------------------------------------- | -------------------- | ----------------------------------------- |
| **Regresi√≥n** (valores continuos)           | `"regression"`       | Minimiza el error cuadr√°tico medio (MSE) |
| **Clasificaci√≥n binaria** (0/1)             | `"binary"`           | Minimiza la p√©rdida logar√≠tmica |
| **Clasificaci√≥n m√∫ltiple** (3 o m√°s clases) | `"multiclass"`       | Usa softmax y logloss para varias clases |
| **Regresi√≥n con outliers**                  | `"huber"`            | Usa funci√≥n Huber para menos |
| **Ranking** (ordenar √≠tems)                 | `"lambdarank"`       | Optimiza una funci√≥n de ranking |

### 2. `metric`

Define c√≥mo se mide el desempe√±o del modelo durante el entrenamiento. Se puede usar una o varias m√©tricas a la vez, y se muestran en cada iteraci√≥n si `verbose > 0`

#### üéì M√©tricas m√°s comunes por tipo de problema

##### üî¢ Regresi√≥n (`objective = "regression"`)

| `metric`  | ¬øQu√© mide?                              |
| --------- | --------------------------------------- |
| `"l2"`    | Error cuadr√°tico medio (MSE)            |
| `"rmse"`  | Ra√≠z del error cuadr√°tico medio         |
| `"mae"`   | Error absoluto medio                    |
| `"huber"` | Error Huber (menos sensible a outliers) |

##### ‚úÖ Clasificaci√≥n binaria (`objective = "binary"`)

| `metric`           | ¬øQu√© mide?                                                      |
| ------------------ | --------------------------------------------------------------- |
| `"binary_logloss"` | P√©rdida logar√≠tmica (predicciones vs probabilidades verdaderas) |
| `"auc"`            | √Årea bajo la curva ROC                                          |
| `"binary_error"`   | Tasa de error (0/1 mal clasificados)                            |

##### üéØ Clasificaci√≥n multiclase (objective = "multiclass")

| `metric`          | ¬øQu√© mide?                                |
| ----------------- | ----------------------------------------- |
| `"multi_logloss"` | P√©rdida logar√≠tmica para m√∫ltiples clases |
| `"multi_error"`   | Porcentaje de predicciones incorrectas    |

### 3. `boosting`

Indica qu√© algoritmo de boosting usar para entrenar el modelo. El m√©todo de boosting impacta en:

```{r boosting, echo=FALSE}
step_style <- "
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  width: 180px;
  min-height: 160px;
  padding: 16px;
  border-radius: 16px;
  background: white;
  border: 2px solid #e5e7eb;
  box-shadow: 4px 6px 14px rgba(0,0,0,0.08);
  transition: transform 0.3s ease, box-shadow 0.3s ease;
"

icon_style <- function(bg, color = "black") {
  paste0(
    "font-size: 32px; background:", bg,
    "; color:", color, "; padding: 12px; border-radius: 50%; margin-bottom: 10px;"
  )
}

steps <- tagList(
  tags$div(
    style = step_style,
    tags$div(style = icon_style("#fee2e2", "#dc2626"), "‚è±Ô∏è"),
    tags$strong(style = "text-align: center;", "Velocidad de entrenamiento")
  ),
  tags$div(
    style = step_style,
    tags$div(style = icon_style("#e0f2fe", "#2563eb"), "üß†"),
    tags$strong(style = "text-align: center;", "Precisi√≥n del modelo")
  ),
  tags$div(
    style = step_style,
    tags$div(style = icon_style("#ecfccb", "#65a30d"), "üíæ"),
    tags$strong(style = "text-align: center;", "Consumo de memoria")
  ),
  tags$div(
    style = step_style,
    tags$div(style = icon_style("#fae8ff", "#a21caf"), "üî•"),
    tags$strong(style = "text-align: center;", "Robustez al overfitting")
  )
)

tags$div(
  style = "
    display: flex;
    justify-content: center;
    flex-wrap: wrap;
    gap: 24px;
    margin-top: 20px;
    font-family: 'Segoe UI', sans-serif;
    font-size: 16px;
  ",
  steps
)

```

LightGBM permite seleccionar entre 4 m√©todos de boosting

| Valor    | ¬øQu√© hace?                                                                   | ¬øCu√°ndo usarlo?                              |
| -------- | ---------------------------------------------------------------------------- | -------------------------------------------- |
| `"gbdt"` | **Gradient Boosted Decision Trees** (por defecto)                            | ‚úîÔ∏è Preciso, funciona bien en casi todo       |
| `"dart"` | Como GBDT, pero **descarta algunos √°rboles al azar**                         | ‚úîÔ∏è Bueno para evitar overfitting             |
| `"goss"` | **Gradient-based One-Side Sampling**: se entrena con un subconjunto de datos | ‚úîÔ∏è M√°s r√°pido, √∫til con muchos datos         |
| `"rf"`   | **Random Forest**: como m√©todo de bagging, no es boosting puro               | üîÅ Solo si necesitas usar √°rboles aleatorios |


### 4. `verbosity`

Controla cu√°nta informaci√≥n imprime LightGBM en consola mientras entrena el modelo.

```{r verbosity, echo=FALSE}
card_style <- "
  display: flex;
  flex-direction: column;
  align-items: center;
  width: 180px;
  padding: 16px;
  border-radius: 16px;
  background: white;
  border: 2px solid #e5e7eb;
  box-shadow: 3px 4px 12px rgba(0,0,0,0.06);
  transition: transform 0.3s ease;
"

icon_style <- function(bg, color = "black") {
  paste0(
    "font-size: 30px; background:", bg,
    "; color:", color,
    "; padding: 10px; border-radius: 50%; margin-bottom: 10px;"
  )
}

verbosity_cards <- tagList(
  tags$div(
    style = card_style,
    tags$div(style = icon_style("#f3f4f6", "#374151"), "üîá"),
    tags$strong("-1"),
    tags$p("No imprime nada. Entrenamiento silencioso.")
  ),
  tags$div(
    style = card_style,
    tags$div(style = icon_style("#fef3c7", "#92400e"), "‚ö†Ô∏è"),
    tags$strong("0"),
    tags$p("Solo muestra errores graves.")
  ),
  tags$div(
    style = card_style,
    tags$div(style = icon_style("#e0f2fe", "#1e40af"), "‚ÑπÔ∏è"),
    tags$strong("1"),
    tags$p("Errores y advertencias (warnings).")
  ),
  tags$div(
    style = card_style,
    tags$div(style = icon_style("#d1fae5", "#047857"), "üîç"),
    tags$strong("2"),
    tags$p("Muestra informaci√≥n del progreso del entrenamiento.")
  ),
  tags$div(
    style = card_style,
    tags$div(style = icon_style("#ede9fe", "#6b21a8"), "üß†"),
    tags$strong("3+"),
    tags$p("Modo detallado. √ötil para debugging profundo.")
  )
)

tags$div(
  style = "
    display: flex;
    justify-content: center;
    flex-wrap: wrap;
    gap: 20px;
    margin-top: 20px;
    font-family: 'Segoe UI', sans-serif;
    font-size: 15px;
  ",
  verbosity_cards
)
```

# Par√°metros de LightGBM (II)

## üå≤ Estructura del √°rbol

### 1. `num_leaves`

Determina cu√°ntas hojas (leaves) puede tener cada √°rbol de decisi√≥n que construye LightGBM. Una hoja es donde termina una decisi√≥n, y contiene la predicci√≥n final para una parte del dataset. Mientras m√°s hojas, m√°s complejo y flexible puede ser el √°rbol.

| Situaci√≥n                    | Valor sugerido      |
| ---------------------------- | ------------------- |
| Dataset peque√±o              | 15‚Äì31 hojas         |
| Dataset grande               | 64‚Äì255 hojas        |
| Muchas variables categ√≥ricas | Menos hojas mejor   |
| Cuando hay overfitting       | Reduce `num_leaves` |

```{r num_leaves, echo=FALSE}
div(
  style = "background: #fff4e5; border-left: 6px solid #f59e0b; padding: 10px; border-radius: 6px; font-size: 22px;",
  HTML("‚ö†Ô∏è <strong>Importante:</strong> <code>num_leaves</code> debe ser <strong>menor que</strong> <code>2^max_depth</code> para evitar <em>overfitting</em>.")
)
```


### 2. `max_depth`

Controla cu√°ntos niveles de decisiones puede tener un √°rbol individual. Si se reduce, se evita que los √°rboles se vuelvan demasiado complejos. Un valor de -1, LightGBM decide por su cuenta (crece hasta donde haga falta).

```{r, echo=FALSE}
library(DiagrammeR)
library(htmltools)

browsable(
  tags$div(
    style = "
      display: flex;
      justify-content: center;
      align-items: center;
      background: transparent;
    ",
    tags$style(HTML("
      body, html, .slide, .content {
        margin: 0 !important;
        padding: 0 !important;
        height: 0% !important;
      }

      svg {
        display: block;
        margin: 0;
        padding: 0;
        height: auto;
        width: 100%;
        max-height: 50vh;
      }

      .grViz {
        display: flex;
        justify-content: center;
        align-items: center;
        padding: 0 !important;
        margin: 0 !important;
        height: auto;
        width: 100%;
        max-height: 50vh;
      }
    ")),
    grViz("
      digraph tree {
        graph [rankdir = TB, margin=0, nodesep=0.6, ranksep=0.6]

        node [shape = box, style = filled, color = none, fontname = Helvetica, fontsize = 13,
              margin = 0.15, fillcolor = '#fefce8']

        edge [arrowhead = vee, color = gray60]

        X1 [label = 'üß† X1 < 5', fillcolor = '#dbeafe']
        X2 [label = 'üß† X2 < 3']
        X3 [label = 'üß† X3 < 7']
        X4 [label = 'üß† X4 < 2']
        Leaf1 [label = 'üçÉ Leaf', fillcolor = '#dcfce7']
        Leaf2 [label = 'üçÉ Leaf', fillcolor = '#dcfce7']
        Leaf3 [label = 'üçÉ Leaf', fillcolor = '#dcfce7']
        Leaf4 [label = 'üçÉ Leaf', fillcolor = '#dcfce7']
        Leaf5 [label = 'üçÉ Leaf', fillcolor = '#dcfce7']

        Nivel0 [label = 'üü¶ Nivel 0', shape = plaintext, fontcolor = '#4b5563', fontsize = 12]
        Nivel1 [label = 'üü© Nivel 1', shape = plaintext, fontcolor = '#4b5563', fontsize = 12]
        Nivel2 [label = 'üü® Nivel 2', shape = plaintext, fontcolor = '#4b5563', fontsize = 12]
        Nivel3 [label = 'üüß Nivel 3', shape = plaintext, fontcolor = '#4b5563', fontsize = 12]

        {rank = same; Nivel0; X1}
        {rank = same; Nivel1; X2; X3}
        {rank = same; Nivel2; Leaf1; Leaf2; X4; Leaf5}
        {rank = same; Nivel3; Leaf3; Leaf4}

        X1 -> X2
        X1 -> X3
        X2 -> Leaf1
        X2 -> Leaf2
        X3 -> X4
        X3 -> Leaf5
        X4 -> Leaf3
        X4 -> Leaf4
      }
    ")
  )
)

```

### 3. `min_data_in_leaf`

Indica la m√≠nima cantidad de observaciones (filas) que debe tener cada hoja en un √°rbol. Una hoja es un grupo final de datos al que el modelo aplica una predicci√≥n. Si una hoja tiene muy pocos datos, puede memorizar el entrenamiento (overfitting). Esto permite controlar la capacidad del √°rbol de especializarse demasiado.

#### ‚öñÔ∏è Recomendaciones pr√°cticas

| Dataset                 | Valor sugerido |
| ----------------------- | -------------- |
| Peque√±o (<10,000 filas) | 10‚Äì50          |
| Mediano                 | 100‚Äì500        |
| Grande                  | 500‚Äì1000 o m√°s |


# Par√°metros de LightGBM (III)

## üê¢ Control del aprendizaje

### 1. `learning_rate`

Define cu√°nto se ajusta el modelo en cada iteraci√≥n (√°rbol). Si el `learning_rate` es muy grande, el modelo aprende r√°pido, pero puede pasarse de largo.

#### üìä Efecto seg√∫n el valor

| `learning_rate`    | Qu√© pasa                                               |
| ------------------ | ------------------------------------------------------ |
| Alto (`‚â• 0.2`)     | Aprende r√°pido, pero puede sobreajustar o divergir     |
| Medio (`0.05‚Äì0.1`) | Equilibrio entre velocidad y precisi√≥n                 |
| Bajo (`< 0.05`)    | Aprende lento pero con mayor control y precisi√≥n final |

```{r learning_rate, echo=FALSE}
div(
  style = paste(
    "background: #fff7ed;",
    "border-left: 6px solid #f97316;",
    "padding: 10px;",
    "border-radius: 6px;",
    "line-height: 1.5;",
    "font-size: 22px;",
    "margin: auto;"
  ),
  HTML("<strong style='color:#c2410c;'>‚ö†Ô∏è Cuidado:</strong><br>"),
  p(HTML("Valores muy altos <code>(learning_rate &gt; 0.3)</code> ‚Üí el modelo puede ser <strong>inestable</strong> y <em>no generalizar bien</em>.")),
  p(HTML("Valores muy bajos <code>(learning_rate &lt; 0.01)</code> ‚Üí el modelo <strong>entrena muy lento</strong> y requiere m√°s <em>fine tuning</em>."))
)
```

### 2. `nrounds`

Define cu√°ntos √°rboles se van a construir durante el entrenamiento. Este par√°metro va fuera de params. Se usa en validaci√≥n con lgb.cv() o lgb.train() si hay conjunto de validaci√≥n.

#### üß™ Recomendaci√≥n pr√°ctica

| `learning_rate` | `nrounds` sugerido |
| --------------- | ------------------ |
| `0.1`           | 100‚Äì500            |
| `0.05`          | 500‚Äì1000           |
| `0.01`          | 1000‚Äì3000          |

```{r nrounds, echo=FALSE}
div(
  style = paste(
    "background: #ecfdf5;",
    "border-left: 6px solid #10b981;",
    "padding: 10px;",
    "border-radius: 6px;",
    "line-height: 1.5;",
    "font-size: 22px;",
    "margin: auto;"
  ),
  HTML("<strong style='color:#047857;'>‚úÖ Tip importante:</strong><br>"),
  p(HTML("üìâ Us√° (<em>early stopping</em>) para evitar entrenar de m√°s. Esto detiene el entrenamiento cuando el modelo <strong>deja de mejorar</strong> sobre un conjunto de validaci√≥n.")),
)
```

# Par√°metros de LightGBM (IV)

## üõë Regularizaci√≥n y generalizaci√≥n

### 1. `early_stopping_round`

Detiene el entrenamiento autom√°ticamente si el modelo no mejora en un n√∫mero determinado de iteraciones consecutivas sobre un conjunto de validaci√≥n.

```{r early_stopping_round, echo=FALSE}
card_style <- "
  display: flex;
  flex-direction: column;
  align-items: center;
  width: 160px;
  background: white;
  border-radius: 12px;
  padding: 16px;
  box-shadow: 2px 2px 8px rgba(0,0,0,0.06);
  transition: transform 0.3s;
  font-size: 22px;
  text-align: center;
"

cards <- tagList(
  tags$div(
    style = card_style,
    tags$span(style = "font-size: 28px;", "‚è±Ô∏è"),
    tags$strong("Ahorra tiempo"),
    "Evita entrenar m√°s rondas de las necesarias."
  ),
  tags$div(
    style = card_style,
    tags$span(style = "font-size: 28px;", "‚ùå"),
    tags$strong("Reduce el sobreajuste"),
    "Se detiene cuando ya no mejora la validaci√≥n."
  ),
  tags$div(
    style = card_style,
    tags$span(style = "font-size: 28px;", "üéØ"),
    tags$strong("Mejora la precisi√≥n"),
    "Ayuda a encontrar el n√∫mero ideal de √°rboles."
  )
)

tags$div(
  style = "display: flex; justify-content: center; gap: 20px; flex-wrap: wrap; margin-top: 20px;",
  cards
)
```

### 2. `bagging_fraction`

Indica el porcentaje de observaciones (filas) usadas para entrenar cada √°rbol. Esto reduce varianza del modelo al hacer que cada √°rbol vea datos distintos. LightGBM utiliza muestreo sin reemplazo en cada √°rbol, de manera de que cada fila solo se utilizara una √∫nica vez en el entrenamiento de cada uno de los arboles de manera aleatoria.

<center>
  <img src="https://www.baeldung.com/wp-content/uploads/sites/4/2023/07/img_64bd9575e6fc8.svg" width="auto" />
</center>

### 3. `feature_fraction`

Indica el porcentaje de columnas (features) seleccionadas aleatoriamente para construir cada √°rbol. Esto introduce variabilidad y reduce la correlaci√≥n entre √°rboles. Es de mucha utilidad cuando hay muchas columnas.

### 4. `min_gain_to_split`

Impide que se creen ramas nuevas si la ganancia no supera cierto umbral, actuando como un filtro para evitar divisiones poco informativas. Esto ayuda a simplificar los √°rboles.

### 5. `lambda_l1`

La regularizaci√≥n L1 (Lasso) castiga la suma de los valores absolutos de las hojas del √°rbol. Esto fuerza la simplicidad empujado pesos de variables poco importantes hacia cero. Suele ser √∫til cuando hay muchas variables poco √∫tiles.

### 6. `lambda_l2`

La regularizaci√≥n L2 (Ridge) penaliza la suma de los cuadrados de los valores en las hojas. Esto evita que las predicciones sean demasiado extremas, ayudandon as√≠ a la estabilidad del modelo.

# Datos para el ejemplo

## Fuente

Los datos provienen del conjunto p√∫blico *Cleveland Heart Disease Database*, disponible en el repositorio de UCI y utilizado en la competencia *"Machine Learning with a Heart"* de [DrivenData](https://www.drivendata.org/competitions/54/machine-learning-with-a-heart) 

## Objetivo 

El objetivo del proyecto es predecir la presencia o ausencia de enfermedad card√≠aca (`heart_disease_present`) a partir de informaci√≥n cl√≠nica b√°sica de cada paciente.

- `heart_disease_present = 0`: no hay cardiopat√≠a presente
- `heart_disease_present = 1`: presencia de una enfermedad card√≠aca

## Variables

En los datos proporcionados hay 14 columnas, donde la columna `patient_id` es un identificador √∫nico y aleatorio y las 13 variables restantes se representan la suguiente informaci√≥n:

- slope_of_peak_exercise_st_segment: pendiente del segmento ST en el electrocardiograma durante el pico del ejercicio f√≠sico. Esta medida indica la calidad del flujo sangu√≠neo al coraz√≥n, ya que una pendiente plana o descendente puede ser un indicador de enfermedad card√≠aca.

  - `slope_of_peak_exercise_st_segment = 1`: Pendiente ascendente (*Up-sloping*)
  - `slope_of_peak_exercise_st_segment = 2`: Pendiente plana (*Flat*) 
  - `slope_of_peak_exercise_st_segment = 3`: Pendiente descendente (*Down-sloping*)

- thal: resultados de una prueba de esfuerzo con talio que mide el flujo sangu√≠neo al coraz√≥n, con valores posibles normal, defecto fijo y defecto reversible.

  - `Normal	`: Flujo sangu√≠neo normal. No se detectan problemas de perfusi√≥n
  - `Fixed Defect`: Puede indicar una zona del coraz√≥n que ya no recibe sangre debido a da√±o permanente.
  - `Reversible Defect`: Indica isquemia, donde el flujo sangu√≠neo se ve comprometido solo durante el esfuerzo.

- resting_blood_pressure: presi√≥n sangu√≠nea en reposo. 

- chest_pain_type: tipo de dolor tor√°cico (4 valores). 

- num_major_vessels: n√∫mero de vasos mayores (0-3) coloreados por fluroscopia.

- fasting_blood_sugar_gt_120_mg_per_dl: az√∫car en sangre en ayunas > 120 mg/dl.

  - `fasting_blood_sugar_gt_120_mg_per_dl = 0`: la glucosa en ayunas es menor o igual a 120 mg/dL
  - `fasting_blood_sugar_gt_120_mg_per_dl = 1`: la glucosa en ayunas es mayor a 120 mg/dL

- resting_ekg_results: resultados electrocardiogr√°ficos en reposo.

  - `resting_ekg_results = 0`: Normal
  - `resting_ekg_results = 1`: Anormalidad de la onda ST-T (problemas de oxigenaci√≥n del coraz√≥n)
  - `resting_ekg_results = 2`: Hipertrofia ventricular izquierda probable

- serum_cholesterol_mg_per_dl: colesterol s√©rico en mg/dl.

- oldpeak_eq_st_depression: depresi√≥n del ST inducida por el ejercicio en relaci√≥n con el reposo. Esta es una medida de anormalidad en los electrocardiogramas. 

- sex: genero.

  - `sex = 0`: mujer
  - `sex = 1`: hombre

- edad: edad en a√±os

- max_heart_rate_achieved: frecuencia cardiaca m√°xima alcanzada (pulsaciones por minuto)

- exercise_induced_angina: presencia de una angina de pecho (dolor o molestia en el pecho) inducida por el ejercicio durante una prueba de esfuerzo card√≠aca.
  
  - `exercise_induced_angina = 0`: Falso
  - `exercise_induced_angina = 1`: Verdadero

# Instalaci√≥n

## CRAN

LightGBM est√° disponible en CRAN y puede instalarse con el siguiente c√≥digo R.

```{r instalacion, eval=FALSE}
install.packages("lightgbm", repos = "https://cran.r-project.org")
```

## GitHub

Cada versi√≥n del paquete que ha estado en CRAN tambi√©n est√° disponible en los [LightGBM releases](https://github.com/microsoft/LightGBM/releases), con un formato de nombre lightgbm-{VERSION}-r-cran.tar.gz

```{r github, eval=FALSE}
devtools::install_github("microsoft/LightGBM", subdir = "R-package")
```

# Ejemplo: librer√≠as

![readr](https://img.shields.io/badge/readr-%2300ADD8.svg?style=flat&logo=r&logoColor=white)
![dplyr](https://img.shields.io/badge/dplyr-%230073AF.svg?style=flat&logo=r&logoColor=white)
![fastDummies](https://img.shields.io/badge/fastDummies-%230073AF.svg?style=flat&logo=r&logoColor=white)
![lightgbm](https://img.shields.io/badge/lightgbm-%230073AF.svg?style=flat&logo=r&logoColor=white)
![pROC](https://img.shields.io/badge/pROC-%230073AF.svg?style=flat&logo=r&logoColor=white)
![ggplot2](https://img.shields.io/badge/ggplot2-%230073AF.svg?style=flat&logo=r&logoColor=white)

```{r librerias}
library(readr)
library(dplyr)
library(summarytools)
library(fastDummies)
library(lightgbm)
library(pROC)
library(ggplot2)
```

# Ejemplo: lectura de datos

```{r wd, include=FALSE, eval=TRUE}
setwd("..")
getwd()
train_values <- read_csv("data/train_values.csv")
train_labels <- read_csv("data/train_labels.csv")
test_values <- read_csv("data/test_values.csv")
test_labels <- read_csv("data/test_labels.csv")
```


```{r lectura, eval=FALSE}
train_values <- read_csv("data/train_values.csv")
train_labels <- read_csv("data/train_labels.csv")
test_values <- read_csv("data/test_values.csv")
test_labels <- read_csv("data/test_labels.csv")
```

## Datos de entrenamiento

```{r train_values_html, echo=FALSE}
train_values_html <- datatable(
  train_values,
  options = list(
    scrollX = TRUE,         # Permite scroll horizontal si hay muchas columnas
    autoWidth = FALSE       # Evita que el ancho se fije autom√°tico y permite expandirse
  ),
  width = '80%',           # Hace que el widget ocupe el 100% del contenedor
)

htmltools::div(
  style = "display: flex; justify-content: center; width: 100%;",
  train_values_html
)
```

## Variable objetivo para entrenamiento

```{r train_labels_html, echo=FALSE}
train_labels_html <- datatable(
  train_labels,
  options = list(
    scrollX = TRUE,         # Permite scroll horizontal si hay muchas columnas
    autoWidth = FALSE       # Evita que el ancho se fije autom√°tico y permite expandirse
  ),
  width = '50%',           # Hace que el widget ocupe el 100% del contenedor
) |>
  tagList(
    tags$div(style = "display: flex; justify-content: center;")
  )

htmltools::div(
  style = "display: flex; justify-content: center; width: 100%;",
  train_labels_html
)
```



# Ejemplo: An√°lisis exploratorio de datos (EDA)

Antes de procesar los datos es recomendable realizar una exploraci√≥n sencilla de la informaci√≥n que ayude a comprender mejor la infomraci√≥n con la que se est√° tranbajando. 

## Variable objetivo

Es de suma importancia conocer si contamos con informaci√≥n desbalanceada para tomar decisiones sobre el muestreo necesario para el entrenamiento.

```{r target}
train_labels |>
  mutate(heart_disease_present = as.factor(heart_disease_present)) |>
  count(heart_disease_present) |>
  ggplot(aes(x = heart_disease_present, y = n, fill = heart_disease_present)) +
  geom_col() +
  labs(title = "Conteo variable respuesta para el set de Train") +
  theme_minimal() + 
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_fill_manual(values = c("#00BA38", "#F8766D"))
```

## Variables categr√≥ricas

Es importante conocer las variables categ√≥ricas que se tienen y considerar su distribuci√≥n. Para variables con 3 o m√°s categor√≠as se recomienda utilizar t√©cnicas de **One Hot Encoding** que ayuden a facilitar el aprendizaje al modelo.

### slope_of_peak_exercise_st_segment

```{r feature_slope_of_peak_exercise_st_segment}
train_values |>
  mutate(slope_of_peak_exercise_st_segment = as.factor(slope_of_peak_exercise_st_segment)) |>
  count(slope_of_peak_exercise_st_segment) |>
  ggplot(aes(x = slope_of_peak_exercise_st_segment, y = n, fill = slope_of_peak_exercise_st_segment)) +
  geom_col() +
  labs(title = "Conteo por slope_of_peak_exercise_st_segment") +
  theme_minimal() + 
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_fill_manual(values = c("#00BA38", "#619CFF", "#F8766D"))
```

### thal

```{r feature_thal}
train_values |>
  mutate(thal = as.factor(thal)) |>
  count(thal) |>
  ggplot(aes(x = thal, y = n, fill = thal)) +
  geom_col() +
  labs(title = "Conteo por thal") +
  theme_minimal() + 
  theme(plot.title = element_text(hjust = 0.5))
```

### chest_pain_type

```{r feature_chest_pain_type}
train_values |>
  mutate(chest_pain_type = as.factor(chest_pain_type)) |>
  count(chest_pain_type) |>
  ggplot(aes(x = chest_pain_type, y = n, fill = chest_pain_type)) +
  geom_col() +
  labs(title = "Conteo por chest pain") +
  theme_minimal() + 
  theme(plot.title = element_text(hjust = 0.5))
```

### fasting_blood_sugar_gt_120_mg_per_dl

```{r feature_fasting_blood_sugar_gt_120_mg_per_dl}
train_values |>
  mutate(fasting_blood_sugar_gt_120_mg_per_dl = as.factor(fasting_blood_sugar_gt_120_mg_per_dl)) |>
  count(fasting_blood_sugar_gt_120_mg_per_dl) |>
  ggplot(aes(x = fasting_blood_sugar_gt_120_mg_per_dl, y = n, fill = fasting_blood_sugar_gt_120_mg_per_dl)) +
  geom_col() +
  labs(title = "Conteo por fasting_blood_sugar_gt_120_mg_per_dlo") +
  theme_minimal() + 
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_fill_manual(values = c("#619CFF", "#F8766D"))
```

### resting_ekg_results

```{r feature_resting_ekg_results}
train_values |>
  mutate(resting_ekg_results = as.factor(resting_ekg_results)) |>
  count(resting_ekg_results) |>
  ggplot(aes(x = resting_ekg_results, y = n, fill = resting_ekg_results)) +
  geom_col() +
  labs(title = "Conteo por chest pain") +
  theme_minimal() + 
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_fill_manual(values = c("#00BA38", "#619CFF", "#F8766D"))
```

### sex

```{r feature_sex}
train_values |>
  mutate(sex = as.factor(sex)) |>
  count(sex) |>
  ggplot(aes(x = sex, y = n, fill = sex)) +
  geom_col() +
  labs(title = "Conteo por Sexo") +
  theme_minimal() + 
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_fill_manual(values = c("#F8766D", "#619CFF"))
```

### exercise_induced_angina

```{r feature_exercise_induced_angina}
train_values |>
  mutate(exercise_induced_angina = as.factor(exercise_induced_angina)) |>
  count(exercise_induced_angina) |>
  ggplot(aes(x = exercise_induced_angina, y = n, fill = exercise_induced_angina)) +
  geom_col() +
  labs(title = "Conteo por exercise_induced_anginao") +
  theme_minimal() + 
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_fill_manual(values = c("#619CFF", "#F8766D"))
```

## Variables continuas

### Estad√≠sticas descriptivas

```{r descriptivos, eval=FALSE}
descr(
  train_values |> dplyr::select(
    resting_blood_pressure, 
    num_major_vessels, 
    serum_cholesterol_mg_per_dl, 
    oldpeak_eq_st_depression, 
    age, 
    max_heart_rate_achieved
  ),
  stats = "common",
  transpose = TRUE,
  round.digits = 2
) |> as.data.frame()
```

```{r descriptivos_html, echo=FALSE}
descriptivos_html <- descr(
  train_values |> dplyr::select(
    resting_blood_pressure, 
    num_major_vessels, 
    serum_cholesterol_mg_per_dl, 
    oldpeak_eq_st_depression, 
    age, 
    max_heart_rate_achieved
  ),
  stats = "common",
  transpose = TRUE,
  round.digits = 2
) |> as.data.frame() |>
  datatable(
    options = list(
      scrollX = TRUE,         # Permite scroll horizontal si hay muchas columnas
      autoWidth = FALSE       # Evita que el ancho se fije autom√°tico y permite expandirse
    ),
    width = '80%',           # Hace que el widget ocupe el 100% del contenedor
  )

htmltools::div(
  style = "display: flex; justify-content: center; width: 100%;",
  descriptivos_html
)
```

### Estad√≠sticas descriptivas por presencia y ausencia de enfermedad cardiaca

```{r}
stby(
  data = train_values |> dplyr::select(
    resting_blood_pressure, 
    num_major_vessels, 
    serum_cholesterol_mg_per_dl, 
    oldpeak_eq_st_depression, 
    age, 
    max_heart_rate_achieved
  ),
  INDICES = train_labels$heart_disease_present,
  FUN = descr,
  stats = "common",
  transpose = TRUE,
  style="rmarkdown" 
)
```

# Ejemplo: preprocesamiento de datos

El preprocesamiento de datos es el paso previo al entrenamiento, donde preparamos los datos para que el modelo pueda aprender de manera efectiva. Se debe limpiar, transformar y codificar los datos para que el modelo aprenda bien.

## Preparaci√≥n de las features de entrenamiento

```{r x_train}
X_train <- train_values |> 
  select(-patient_id) |>             # Se elimina la columna ID ya que no aporta info
  mutate(thal = as.factor(thal)) |>  # Convertimos 'thal' a factor (categ√≥rica)
  dummy_cols(
    select_columns = "thal",
    remove_selected_columns = TRUE
  ) |>                               # Creamos columnas dummy para 'thal'
  as.matrix()                        # Convertimos el data frame a matriz
```

## Preparaci√≥n de la variable objetivo o target

```{r y_train}
y_train <- train_labels |> 
  select(heart_disease_present) |> 
  as.matrix()
```

## Se repite el proceso para el conjunto de validaci√≥n

```{r test}
X_test <- test_values |> 
  select(-patient_id) |> 
  mutate(thal = as.factor(thal)) |> 
  dummy_cols(
    select_columns = "thal", 
    remove_selected_columns = TRUE
  ) |>
  as.matrix()

y_test <- test_labels |> 
  select(heart_disease_present) |> 
  as.matrix()
```

## Creaci√≥n de los datasets de LightGBM

```{r lgb_dataset}
dtrain <- lgb.Dataset(data = X_train, label = y_train)
dtest  <- lgb.Dataset(data = X_test, label = y_test)
```

# Ejemplo: entrenamiento

```{r train}
params <- list(
  objective = "binary",
  metric = "auc",
  boosting = "gbdt",
  verbosity = 3,
  num_leaves = 31,
  learning_rate = 0.05,
  bagging_fraction = 0.9,
  feature_fraction = 0.9,
  bagging_freq = 1,
  seed = 123456,
  bagging_seed = 123456,
  feature_fraction_seed = 123456
)

model <- lgb.train(
  params = params,
  nrounds = 100,
  data = dtrain,
  valids = list(train = dtrain, test = dtest),
  early_stopping_rounds = 10,
  record = TRUE
)
```

# Ejemplo: c√°lculo del gini

El √≠ndice de Gini es una m√©trica de performance para modelos de clasificaci√≥n binaria que mide qu√© tan bien separa el modelo las clases.

El √≠ndice de Gini se calcula a partir del AUC de la curva ROC:

$$
Gini = 2 \times AUC - 1
$$

```{r gini, fig.align='center'}
# C√°lculo de ginis
auc_train <- model$record_evals$train$auc$eval
auc_test <- model$record_evals$test$auc$eval
gini_train <- 2 * unlist(auc_train) - 1
gini_test <- 2 * unlist(auc_test) - 1

# Data frame con los ginis
gini_df <- data.frame(
  iter = seq_along(gini_train),
  train = gini_train,
  test = gini_test
)

# Gr√°fico de ginis
ggplot(gini_df, aes(x = iter)) +
  ylim(0.5, 1) +
  geom_line(aes(y = train, color = "Train"), size = 1.2) +
  geom_line(aes(y = test, color = "Test"), size = 1.2) +
  scale_color_manual(values = c("Train" = "#1f77b4", "Test" = "#ff7f0e")) +
  labs(
    title = "Evolucion del Gini por ronda",
    x = "Iteracion",
    y = "Gini",
    color = "Conjunto"
  )+
  theme_minimal() + 
  theme(plot.title = element_text(hjust = 0.5))
```

# Ejemplo: curva ROC

La curva ROC (Receiver Operating Characteristic) muestra la capacidad de un modelo binario para distinguir entre clases positivas y negativas.

```{r roc, echo=FALSE}
div(
  style = paste(
    "background: #f9fafb;",
    "border-left: 6px solid #3b82f6;",  # azul claro
    "padding: 16px;",
    "border-radius: 8px;",
    "font-family: sans-serif;",
    "margin-top: 20px;",
    "line-height: 1.6;"
  ),
  h3("üîç ¬øQu√© representa la curva ROC?"),
  tags$ul(
    tags$li(HTML("<strong>Eje Y:</strong> Sensibilidad (True Positive Rate) ‚Üí ¬øCu√°ntos positivos detectamos correctamente?")),
    tags$li(HTML("<strong>Eje X:</strong> 1 - Especificidad (False Positive Rate) ‚Üí ¬øCu√°ntos negativos estamos clasificando mal?")),
    tags$li(HTML("<strong>Diagonal:</strong> Clasificador aleatorio (sin poder predictivo)")),
    tags$p(HTML("Cuanto m√°s arriba y a la izquierda, mejor el modelo"))
  )
)
```


```{r roc_auc}
# C√°lculo de la curva ROC
roc_train <- roc(y_train, predict(model, X_train))
roc_test  <- roc(y_test, predict(model, X_test))

# Data frames con las curvas ROC de train y test
df_train <- data.frame(
  tpr = rev(roc_train$sensitivities),
  fpr = rev(1 - roc_train$specificities),
  dataset = "Train"
)
df_test <- data.frame(
  tpr = rev(roc_test$sensitivities),
  fpr = rev(1 - roc_test$specificities),
  dataset = "Test"
)

# Uni√≥n de data frames
roc_df <- rbind(df_train, df_test)

# Gr√°fico AUC ROC
ggplot(roc_df, aes(x = fpr, y = tpr, color = dataset)) +
  geom_line(size = 1.2) +
  scale_color_manual(values = c("Train" = "#1f77b4", "Test" = "#ff7f0e")) +
  geom_abline(linetype = "dashed", color = "gray") +
  labs(
    title = "Curva ROC",
    x = "1 - Especificidad",
    y = "Sensibilidad"
  )+
  theme_minimal() + 
  theme(plot.title = element_text(hjust = 0.5))
```

# Ejemplo: matriz de confusi√≥n

La matriz de confusi√≥n muestra c√≥mo se comporta el modelo al predecir. Las celdas diagonales indican predicciones correctas. Nos permite identificar cu√°ntos falsos positivos y falsos negativos hay, lo cual es clave cuando el costo de equivocarse es diferente seg√∫n el caso.

```{r matriz}
# Test
df_conf_matrix_test  <- table(
    Predicted = predict(model, X_test) |> round(),  
    Actual = y_test
  ) |> 
  as.data.frame() |>
  mutate(
    Predicted = factor(Predicted, levels = c("1", "0")),
    Actual = factor(Actual, levels = c("0", "1")),
  ) 

# Plot matriz de confusi√≥n de Test
ggplot(df_conf_matrix_test, aes(x = Actual, y = Predicted, fill = Freq)) +
  geom_tile(color = "white") +
  geom_text(aes(label = Freq), size = 6) +
  scale_fill_gradient(low = "white", high = "#badb33") +
  labs(title = "Matriz de confusi√≥n", x = "Valor real", y = "Predicci√≥n") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))
```

# Ejemplo: m√©tricas de clasificaci√≥n

Para el c√°lculo de las m√©tricas es necesario partir de la matriz de confusi+on generada de forma similar a la diapositiva anterior. 

```{r conf_matrix_all}
# Train
conf_matrix_train <- table(
  Actual    = y_train,
  Predicted = predict(model, X_train) |> round()
) |> as.matrix()

# Test
conf_matrix_test <- table(
  Actual    = y_test,
  Predicted = predict(model, X_test) |> round()
) |> as.matrix()
```

## Accuracy

Mide la proporci√≥n de predicciones correctas (positivas y negativas).

$$
\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
$$

```{r accuracy}
accuracy_train <- sum(diag(conf_matrix_train)) / sum(conf_matrix_train)
accuracy_test  <- sum(diag(conf_matrix_test))  / sum(conf_matrix_test)
```

## Precision
Indica de todos los positivos predichos, cu√°ntos eran realmente positivos.

$$
\text{Precision} = \frac{TP}{TP + FP}
$$

```{r precision}
precision_train <- conf_matrix_train["1","1"] / sum(conf_matrix_train["1",])
precision_test  <- conf_matrix_test["1","1"]   / sum(conf_matrix_test["1",])
```

## Recall

Indica de todos los positivos reales, cu√°ntos se detectaron correctamente.

$$
\text{Recall} = \frac{TP}{TP + FN}
$$

```{r recall}
recall_train <- conf_matrix_train["1","1"] / sum(conf_matrix_train[,"1"])
recall_test  <- conf_matrix_test["1","1"]   / sum(conf_matrix_test[,"1"])
```

## F1-Score

EIndica un equilibrio entre Precision y Recall. Es su media arm√≥nica.

$$
\text{F1} = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
$$

```{r f1_score}
f1_train <- 2 * precision_train * recall_train / (precision_train + recall_train)
f1_test  <- 2 * precision_test * recall_test / (precision_test + recall_test)
```

## Resultados

```{r metrics}
metrics <- data.frame(
  Dataset = c("Train", "Test"),
  Accuracy = c(
    round(accuracy_train * 100, 2), 
    round(accuracy_test * 100, 2)
  ),
  Precision = c(
    round(precision_train * 100, 2), 
    round(precision_test * 100, 2)
  ),
  Recall = c(
    round(recall_train * 100, 2), 
    round(recall_test * 100, 2)
  ),
  F1 = c(
    round(f1_train, 4), 
    round(f1_test, 4)
  )
)
```

```{r metrics_table, echo=FALSE}
library(kableExtra)
metrics %>%
  kable(caption = "M√©tricas de Evaluaci√≥n", format = "html") %>%
  kable_styling(position = "center", full_width = FALSE) %>%
  row_spec(0, bold = TRUE)
```

# Ejemplo: feature importance

El feature importance de lightgbm muestra 3 columnas:

- Gain: medida del incremento en la funci√≥n objetivo cuando la variable se usa para hacer un split en los √°rboles. En t√©rminos simples, indica cu√°nto ayuda esa variable a mejorar el modelo.

- Cover: representa la proporci√≥n de observaciones que caen en nodos donde la variable se us√≥ para dividir. Es decir, cu√°nto ‚Äúcubre‚Äù esa variable en el dataset durante el entrenamiento.

- Frequency: indica el n√∫mero relativo de veces que esa variable fue utilizada para dividir (split) en todos los √°rboles del modelo. M√°s frecuencia indica que el modelo recurri√≥ m√°s a esa variable para hacer predicciones.

```{r feature_importance}
lgb.importance(model)
```

LightGBM tambi√©n permite realizar un gr√°fico del feature importance.

```{r plot_fi}
lgb.importance(model) |> 
  lgb.plot.importance(top_n = 20)
```

# Ejemplo: consideraciones finales

## Guardar el modelo entrenado

Una vez entrenado el modelo, es recomendable guardarlo para no tener que repetir el proceso de entrenamiento. Esto es √∫til cuando queremos reutilizar el modelo en futuras predicciones.

```{r lbg_save}
lgb.save(model, "modelo_lgbm.txt")
```

## Cargar el modelo previamente guardado

Una vez guardado un modelo, podemos volver a cargarlo de manera similar a como se cargan los dataframes. Esto es especialmente √∫til para usar el modelo en otros scripts, sesiones, o despu√©s de reiniciar R.

```{r lgb_load}
modelo_cargado <- lgb.load("modelo_lgbm.txt")
```

## Verificar veracidad del modelo

Es posible comprobar que el modelo cargado produce los mismos resultados que el original.

```{r all_equal}
all.equal(
  predict(model, X_test),
  predict(modelo_cargado, X_test)
)
```

