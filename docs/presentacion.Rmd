---
title: "Introducci√≥n a LightGBM con R"
author: "Carlos A. Torres Cubilla"
output:
  slidy_presentation: 
    number_sections: false
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: false
---

```{r setup, include=FALSE}
library(htmltools)
library(DiagrammeR)
knitr::opts_chunk$set(
  echo = TRUE, 
  message = FALSE, 
  warning = FALSE,
  fig.align='center'
)
```

# ¬øQu√© es LightGBM?

LightGBM es un framework de *gradient boosting* que utiliza algoritmos de aprendizaje basados en √°rboles para crear modelos predictivos de manera eficiente y r√°pida. 

## ¬øQue significa LightGBM?

- üî¶ **Light** = ligero, r√°pido, optimizado
  - Se refiere a que es una versi√≥n optimizada del algoritmo de Gradient Boosting.
  - Est√° dise√±ado para consumir menos memoria y funcionar m√°s r√°pido que alternativas como XGBoost o Random Forests.
- üìà **GBM** = *Gradient Boosting Machine*
  - Un tipo de t√©cnica de ensemble learning que genera muchos modelos d√©biles (generalmente √°rboles de decisi√≥n) de forma secuencial, donde cada modelo aprende de los errores de sus predecesores para crear un modelo fuerte .
  - Se basa en usar el gradiente del error para mejorar paso a paso.

<div style="text-align: center;">
  <img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiN2KPoea9rFZo4nb0SZKrBrEUjNv-xaqB7gF6Htl5lY5AtOmKH1yFalD9Y6XHNNgtUYqsJCPUr-7a4MJIvdcubXogxerrskVqKfQGhKSpUyrnroLhEi6P5vMXqYE22J3_dnLRuWiBv5Nw/s0/Random+Forest+03.gif" style="display: block; margin-left: auto; margin-right: auto;" width="75%"/>
</div>

<div class="gradient-boosting" style="background-color: #f9f9f9; border-left: 5px solid #007ACC; padding: 10px; margin-top: 0px;">
  LightGBM mejora los errores paso a paso combinando √°rboles de decisi√≥n peque√±os.
</div>

## ¬øQui√©n cre√≥ LightGBM?

<div style="display: flex; align-items: center; gap: 1rem;">
  <img src="https://upload.wikimedia.org/wikipedia/commons/9/96/Microsoft_logo_%282012%29.svg" 
       alt="Microsoft Logo" 
       style="height: 50px; margin-bottom: 0;" />

<div>
  <p style="margin: 0;">
    Desarrollado por <strong>Microsoft Research Asia</strong>  
    como parte del toolkit <strong>DMTK</strong> (*Distributed Machine Learning Toolkit*).
  </p>
  <p style="margin: 0;">
    Lanzado en <strong>2016</strong> como proyecto <em>open source</em>.
  </p>
</div>
</div>

> <em>"Creamos LightGBM para resolver problemas del mundo real a gran escala con eficiencia."</em>  
> ‚Äî <strong>Microsoft DMTK Team</strong>

# ¬øC√≥mo funciona el Gradient Boosting?

Podemos resumir el funcionamiento del *Gradient Boosting* en los siguientes 5 pasos: 

```{r boosting_paso_a_paso, echo=FALSE}
step_style <- "
  display: flex;
  flex-direction: column;
  align-items: center;
  width: 160px;
  border: 1px solid #ccc;
  border-radius: 12px;
  padding: 12px;
  box-shadow: 2px 2px 8px rgba(0,0,0,0.1);
  transition: transform 0.3s;
"

steps <- tagList(
  tags$div(
    style = step_style,
    tags$span(style = "font-size: 30px;", "üìÖ"),
    tags$strong("1. Predicci√≥n Inicial"),
    tags$p("Promedio de la variable objetivo o log-odds si es clasificaci√≥n.")
  ),
  tags$div(
    style = step_style,
    tags$span(style = "font-size: 30px;", "üìà"),
    tags$strong("2. C√°lculo de Error"),
    tags$p("Se calcula la diferencia entre la predicci√≥n y el valor real.")
  ),
  tags$div(
    style = step_style,
    tags$span(style = "font-size: 30px;", "üå≥"),
    tags$strong("3. Nuevo √°rbol"),
    tags$p("Se entrena un √°rbol sobre los errores residuales (gradientes).")
  ),
  tags$div(
    style = step_style,
    tags$span(style = "font-size: 30px;", "‚öñÔ∏è"),
    tags$strong("4. Ponderar"),
    tags$p("Se multiplica por el learning rate para suavizar el ajuste.")
  ),
  tags$div(
    style = step_style,
    tags$span(style = "font-size: 30px;", "‚ûï"),
    tags$strong("5. Sumar"),
    tags$p("Se agrega el √°rbol al modelo existente.")
  )
)

tags$div(
  style = "display: flex; justify-content: center; gap: 20px; flex-wrap: wrap; margin-top: 20px;",
  steps
)
```


üîÑ El proceso se repite muchas veces, agregando un √°rbol nuevo en cada paso, hasta que se alcanza el n√∫mero m√°ximo de rondas (nrounds) o se activa un criterio de parada temprana (early_stopping_rounds) si el rendimiento en el conjunto de validaci√≥n deja de mejorar.

<center>
  <img src="https://media.geeksforgeeks.org/wp-content/uploads/20250519125344578128/python.webp" width="800" />
</center>

Estos pasos se pueden expresar de manera matem√°tica de manera facil

### Paso 1: Predicci√≥n Inicial

El modelo comienza con una predicci√≥n constante para todos los datos.

- En regresi√≥n, suele ser: \(F_0(x) = \bar{y} \)

- En clasificaci√≥n binaria, suele ser: \( F_0(x) = \log\left(\frac{p}{1 - p}\right), \quad p = \frac{\text{# de clase 1}}{n} \)

### Paso 2: Calcular el Error o Gradiente

Para cada observaci√≥n, se calcula cu√°nto se est√° equivocando el modelo actual:

\( g_i^{1} = y_i - F_{0}(x_i) \)

Este paso corresponde al c√°lculo del gradiente de la funci√≥n de p√©rdida con respecto a la predicci√≥n actual.

### Paso 3: Entrenar un nuevo modelo sobre ese error

Se entrena un modelo d√©bil que aprenda a corregir esos errores: \( h_1(x) \approx g_i^{1} \)

### Paso 4: Escalar la correcci√≥n con un Learning Rate
Se ajusta la magnitud de la correcci√≥n aplicando un factor de aprendizaje \( \eta \) (por ejemplo, 0.1):

\( \text{correcci√≥n} = \eta \cdot h_1(x) \)

Esto suaviza el aprendizaje y previene el sobreajuste.

### Paso 5: Actualizar el modelo

Se suma la correcci√≥n al modelo acumulado anterior:

\( F_m(x) = F_0(x) + \eta \cdot h_1(x) \)

Este proceso se repite durante varias iteraciones, agregando un nuevo modelo d√©bil \( h(x) \) cada vez, hasta alcanzar un n√∫mero m√°ximo de rondas (*nrounds*) o un criterio de parada temprana (*early_stopping_rounds*)

### Resultado final

La predicci√≥n final del modelo se puede representar mediante la siguiente funci√≥n:

\( F_m(x) = F_0(x) + \sum_{m=1}^M \eta \cdot h_m(x)\)

Donde: 

- \( F_m(x) \): predicci√≥n final,
- \( \eta \): learning rate,
- \( h_m(x) \): modelo d√©bil,
- \( M \): n√∫mero total de iteraciones.

En la siguiente imagen se representa visualmente el proceso de mejora paso a paso de un modelo basado en *Gradien Boosting*. 

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.align='center'}
library(lightgbm)
library(ggplot2)
library(dplyr)

# Datos simulados
set.seed(123)
n <- 300
x <- runif(n, 0, 10)
y <- sin(x) + rnorm(n, 0, 0.3)
data <- data.frame(x = x, y = y)

# Dataset para LightGBM
X_mat <- matrix(x, ncol = 1)
dtrain <- lgb.Dataset(data = X_mat, label = y)

# Modelo completo con 10 iteraciones
params <- list(
  objective = "regression",
  learning_rate = 0.3,
  num_leaves = 4,
  max_depth = 2,
  verbosity = -1
)
model <- lgb.train(params, dtrain, nrounds = 12)

# Predicciones paso a paso
predictions <- lapply(1:12, function(i) {
  data.frame(
    x = x,
    y_pred = predict(model, X_mat, num_iteration = i),
    paso = paste("Paso", i)
  )
}) %>% bind_rows()

# Convertir a factor con niveles ordenados
predictions$paso <- factor(predictions$paso, levels = paste("Paso", 1:12))

# Visualizaci√≥n
ggplot(predictions, aes(x = x, y = y_pred)) +
  geom_line(color = "blue", alpha = 0.7) +
  geom_point(data = data, aes(x = x, y = y), alpha = 0.2) +
  facet_wrap(~ paso, ncol = 4) +
  theme_minimal(base_size = 12) +
  labs(title = "Gradient Boosting: Mejora paso a paso", x = "x", y = "Predicci√≥n") +
  theme(plot.title = element_text(hjust = 0.5)) 
```

Cada iteraci√≥n mejora el modelo poco a poco, corrigiendo errores anteriores.

# Ventajas y desventajas

LightGBM se destaca no solo por su rendimiento predictivo, sino tambi√©n por su eficiencia computacional. Su adopci√≥n generalizada en aplicaciones reales de machine learning se puede explicar mediante cinco ventajas clave.

## Ventajas

```{r ventajas, echo=FALSE}
ventajas_style <- "
  display: flex; 
  flex-direction: column; 
  align-items: center; 
  width: 120px; 
  transition: transform 0.3s;
"

tags$div(
  style = "display: flex; justify-content: center; gap: 40px; margin-top: 20px;",

  tags$div(
    style = ventajas_style,
    onmouseover = sprintf("this.style.transform='scale(1.1)'"),
    onmouseout = sprintf("this.style.transform='scale(1)'"),
    tags$span(style = "font-size: 40px;", "‚ö°"),
    tags$p("Velocidad")
  ),

  tags$div(
    style = ventajas_style,
    onmouseover = sprintf("this.style.transform='scale(1.1)'"),
    onmouseout = sprintf("this.style.transform='scale(1)'"),
    tags$span(style = "font-size: 40px;", "üíæ"),
    tags$p("Memoria")
  ),

  tags$div(
    style = ventajas_style,
    onmouseover = sprintf("this.style.transform='scale(1.1)'"),
    onmouseout = sprintf("this.style.transform='scale(1)'"),
    tags$span(style = "font-size: 40px;", "üéØ"),
    tags$p("Precisi√≥n")
  ),

  tags$div(
    style = "display: flex; flex-direction: column; align-items: center; width: 120px; transition: transform 0.3s;",
    onmouseover = sprintf("this.style.transform='scale(1.1)'"),
    onmouseout = sprintf("this.style.transform='scale(1)'"),
    tags$span(style = "font-size: 40px;", "üñ•Ô∏è"),
    tags$p("Paralelismo")
  ),

  tags$div(
    style = ventajas_style,
    onmouseover = sprintf("this.style.transform='scale(1.1)'"),
    onmouseout = sprintf("this.style.transform='scale(1)'"),
    tags$span(style = "font-size: 40px;", "üìä"),
    tags$p("Escalabilidad")
  )
)
```

<div style="margin: 0 auto; width: fit-content;">
| Ventaja       | Descripci√≥n                                                                 |
|---------------|------------------------------------------------------------------------------|
| Velocidad   | Entrena modelos r√°pidamente, lo que lo hace ideal para proyectos con grandes vol√∫menes de datos |
| Memoria     | Optimiza el consumo de memoria, permitiendo trabajar con datasets grandes sin requerir tanta RAM         |
| Precisi√≥n   | Proporciona resultados precisos y competitivos en tareas de predicci√≥n                    |
|  Paralelismo | Aprovecha m√∫ltiples n√∫cleos, cl√∫steres y aceleraci√≥n por GPU para entrenamientos m√°s r√°pidos            |
| Escalabilidad | Maneja grandes vol√∫menes de datos sin p√©rdida de rendimiento       |
</div>

## Desventajas

```{r desventajas, echo=FALSE}
desventajas_style <- "
  display: flex; 
  flex-direction: column; 
  align-items: center; 
  width: 130px; 
  transition: transform 0.3s;
"

tags$div(
  style = "display: flex; justify-content: center; flex-wrap: wrap; gap: 40px; margin-top: 20px;",

  tags$div(
    style = desventajas_style,
    onmouseover = "this.style.transform='scale(1.1)'",
    onmouseout = "this.style.transform='scale(1)'",
    tags$span(style = "font-size: 40px;", "‚ö†Ô∏è"),
    tags$p("Overfitting")
  ),

  tags$div(
    style = desventajas_style,
    onmouseover = "this.style.transform='scale(1.1)'",
    onmouseout = "this.style.transform='scale(1)'",
    tags$span(style = "font-size: 40px;", "üîç"),
    tags$p("Dif√≠cil de interpretar")
  ),

  tags$div(
    style = desventajas_style,
    onmouseover = "this.style.transform='scale(1.1)'",
    onmouseout = "this.style.transform='scale(1)'",
    tags$span(style = "font-size: 40px;", "üìâ"),
    tags$p("Problemas en datasets peque√±os")
  ),

  tags$div(
    style = desventajas_style,
    onmouseover = "this.style.transform='scale(1.1)'",
    onmouseout = "this.style.transform='scale(1)'",
    tags$span(style = "font-size: 40px;", "üß†"),
    tags$p("Curva de aprendizaje")
  ),

  tags$div(
    style = desventajas_style,
    onmouseover = "this.style.transform='scale(1.1)'",
    onmouseout = "this.style.transform='scale(1)'",
    tags$span(style = "font-size: 40px;", "üîå"),
    tags$p("Soporte parcial en entornos")
  )
)
```

<div style="margin: 0 auto; width: fit-content;">
| ‚ö†Ô∏è Desventaja                | üìù Descripci√≥n                                                                 |
|-----------------------------|--------------------------------------------------------------------------------|
| Overfitting                 | Al ser muy poderoso, puede memorizar ruido si no se ajusta correctamente.      |
| Dif√≠cil de interpretar      | Es una "caja negra" frente a modelos m√°s simples como regresi√≥n lineal.        |
| Problemas en datasets peque√±os | Modelos simples pueden generalizar mejor en conjuntos de datos chicos.          |
| Curva de aprendizaje        | Muchos hiperpar√°metros que requieren experiencia para ajustar bien.            |
| Soporte parcial en entornos | APIs disponibles, pero algunas integraciones pueden ser limitadas o inestables. |
</div>



# ¬øPara qu√© utilizar LightGBM?

LightGBM es  altamente vers√°til que permite resolver una amplia variedad de problemas, desde tareas tradicionales como clasificaci√≥n y regresi√≥n, hasta aplicaciones m√°s complejas como ranking, detecci√≥n de fraude y sistemas de recomendaci√≥n personalizados. Esto lo convierte en una herramienta poderosa y escalable para proyectos de aprendizaje autom√°tico en entornos reales.

```{r uso, echo=FALSE}
browsable(
  tags$div(
    style = "
      display: flex;
      justify-content: center;
      align-items: center;
      background: transparent;
    ",
    tags$style(HTML("
      /* Reset total de margenes/paddings en Slidy */
      body, html, .slide, .content {
        margin: 0 !important;
        padding: 0 !important;
        height: 0% !important;
      }

      /* El SVG generado por DiagrammeR */
      svg {
        display: block;
        margin: 0;
        padding: 0;
        height: auto;
        width: 0;
        max-height: 30vh;
      }

      .grViz {
        display: flex;
        justify-content: center;
        align-items: center;
        padding: 0 !important;
        margin: 0 !important;
        height: auto;
        width: 0;
        max-height: 30vh;
      }
      }
    ")),
    grViz("
      digraph lightgbm {
        graph [rankdir = TB, margin=0, nodesep=0.3, ranksep=0.3]
        node [shape = box, style = filled, fillcolor = LightGray, fontsize = 12, margin=0.05]

        Usos [label = '¬øPara qu√© se usa?', shape=box, fillcolor=lightblue]
        Clasificacion [label = 'Clasificaci√≥n']
        Regresion [label = 'Regresi√≥n']
        Ranking [label = 'Ranking']
        DeteccionFraude [label = 'Detecci√≥n de\\nfraude']
        Recomendacion [label = 'Sistemas de\\nrecomendaci√≥n']

        Usos -> Clasificacion
        Usos -> Regresion
        Usos -> Ranking
        Usos -> DeteccionFraude
        Usos -> Recomendacion
      }
    ")
  )
)
```

Much√≠simas empresas en todo el mundo utilizan LightGBM debido a su velocidad y precisi√≥n para resolver distintos problemas de machine learning. Algunos ejemplos de compa√±√≠as l√≠deres que han implementado LightGBM en sus soluciones para estos casos de uso:

```{r empresas, echo=FALSE}
html <- tags$div(
  # Estilo del contenedor general
  style = "display: flex; justify-content: center; flex-wrap: wrap; gap: 40px; margin-top: 20px;",
  
  # Estilo para flip cards
  tags$style(HTML("
    .flip-card {
      background-color: transparent;
      width: 175px;
      height: 250px;
      perspective: 1000px;
    }

    .flip-card-inner {
      position: relative;
      width: 100%;
      height: 100%;
      transform-style: preserve-3d;
      transition: transform 0.6s ease;
      transform-origin: center center;
    }

    .flip-card:hover .flip-card-inner {
      transform: rotateY(180deg);
    }

    .flip-card-front, .flip-card-back {
      position: absolute;
      width: 100%;
      height: 100%;
      backface-visibility: hidden;
      border-radius: 10px;
      box-shadow: 0 4px 8px rgba(0,0,0,0.1);
      display: flex;
      flex-direction: column;
      align-items: center;
      justify-content: center;
      padding: 10px;
      box-sizing: border-box;
      text-align: center;
      overflow: hidden;
    }

    .flip-card-front {
      background: white;
    }

    .flip-card-back {
      background-color: #f0f0f0;
      transform: rotateY(180deg);
      font-size: 0.75em;
    }

    .flip-card img {
      width: 90px;
      height: auto;
      margin-bottom: 10px;
    }
    
  ")),

  # Contenido de las tarjetas
  lapply(list(
    list(nombre = "Microsoft", uso = "Clasificaci√≥n", logo = "https://upload.wikimedia.org/wikipedia/commons/9/96/Microsoft_logo_%282012%29.svg", detalle = "Categorizar correos y detectar spam"),
    list(nombre = "Uber", uso = "Regresi√≥n", logo = "https://upload.wikimedia.org/wikipedia/commons/5/58/Uber_logo_2018.svg", detalle = "Estimar demanda y tiempos de llegada"),
    list(nombre = "LinkedIn", uso = "Ranking", logo = "https://upload.wikimedia.org/wikipedia/commons/thumb/a/aa/LinkedIn_2021.svg/2560px-LinkedIn_2021.svg.png", detalle = "Ordenar resultados de b√∫squeda y recomendaciones de empleo"),
    list(nombre = "PayPal", uso = "Detecci√≥n de fraude", logo = "https://upload.wikimedia.org/wikipedia/commons/b/b5/PayPal.svg", detalle = "Detectar transacciones sospechosas en tiempo real"),
    list(nombre = "Netflix", uso = "Sistemas de recomendaci√≥n", logo = "https://upload.wikimedia.org/wikipedia/commons/0/08/Netflix_2015_logo.svg", detalle = "Personalizar sugerencias de contenido")
  ), function(e) {
    tags$div(class = "flip-card",
      tags$div(class = "flip-card-inner",
        tags$div(class = "flip-card-front",
          tags$img(src = e$logo),
          tags$p(e$uso, style = "color: gray; font-size: 0.85em;")
        ),
        tags$div(class = "flip-card-back",
          tags$strong(e$nombre),
          tags$p(e$detalle)
        )
      )
    )
  })
)

browsable(html)
```

# Par√°metros de LightGBM (I)

Cuando entrenamos un modelo con lightgbm, necesitamos decirle c√≥mo aprender. Los par√°metros del modelo le indican a LightGBM como y que tanto aprender. Para una informaci√≥n m√°s completa de los par√°metros de LightGBM visitar la [documentaci√≥n oficial en R](https://lightgbm.readthedocs.io/en/latest/R/index.html).

## ‚öôÔ∏è Par√°metros generales

### 1. `objective`

Es el par√°metro m√°s importante de todos. Le dice a LightGBM qu√© tipo de problema est√°s resolviendo para que use la funci√≥n de p√©rdida adecuada durante el entrenamiento. 

#### üéì Tipos de `objective` m√°s comunes

| Tipo de problema                            | Valor de `objective` | ¬øQu√© hace internamente? |
| ------------------------------------------- | -------------------- | ----------------------------------------- |
| **Regresi√≥n** (valores continuos)           | `"regression"`       | Minimiza el error cuadr√°tico medio (MSE) |
| **Clasificaci√≥n binaria** (0/1)             | `"binary"`           | Minimiza la p√©rdida logar√≠tmica |
| **Clasificaci√≥n m√∫ltiple** (3 o m√°s clases) | `"multiclass"`       | Usa softmax y logloss para varias clases |
| **Regresi√≥n con outliers**                  | `"huber"`            | Usa funci√≥n Huber para menos |
| **Ranking** (ordenar √≠tems)                 | `"lambdarank"`       | Optimiza una funci√≥n de ranking |

### 2. `metric`

Define c√≥mo se mide el desempe√±o del modelo durante el entrenamiento. Se puede usar una o varias m√©tricas a la vez, y se muestran en cada iteraci√≥n si `verbose > 0`

#### üéì M√©tricas m√°s comunes por tipo de problema

##### üî¢ Regresi√≥n (`objective = "regression"`)

| `metric`  | ¬øQu√© mide?                              |
| --------- | --------------------------------------- |
| `"l2"`    | Error cuadr√°tico medio (MSE)            |
| `"rmse"`  | Ra√≠z del error cuadr√°tico medio         |
| `"mae"`   | Error absoluto medio                    |
| `"huber"` | Error Huber (menos sensible a outliers) |

##### ‚úÖ Clasificaci√≥n binaria (`objective = "binary"`)

| `metric`           | ¬øQu√© mide?                                                      |
| ------------------ | --------------------------------------------------------------- |
| `"binary_logloss"` | P√©rdida logar√≠tmica (predicciones vs probabilidades verdaderas) |
| `"auc"`            | √Årea bajo la curva ROC                                          |
| `"binary_error"`   | Tasa de error (0/1 mal clasificados)                            |

##### üéØ Clasificaci√≥n multiclase (objective = "multiclass")

| `metric`          | ¬øQu√© mide?                                |
| ----------------- | ----------------------------------------- |
| `"multi_logloss"` | P√©rdida logar√≠tmica para m√∫ltiples clases |
| `"multi_error"`   | Porcentaje de predicciones incorrectas    |

### 3. `boosting`

Indica qu√© algoritmo de boosting usar para entrenar el modelo. El m√©todo de boosting impacta en:

```{r boosting, echo=FALSE}
step_style <- "
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  width: 180px;
  min-height: 160px;
  padding: 16px;
  border-radius: 16px;
  background: white;
  border: 2px solid #e5e7eb;
  box-shadow: 4px 6px 14px rgba(0,0,0,0.08);
  transition: transform 0.3s ease, box-shadow 0.3s ease;
"

icon_style <- function(bg, color = "black") {
  paste0(
    "font-size: 32px; background:", bg,
    "; color:", color, "; padding: 12px; border-radius: 50%; margin-bottom: 10px;"
  )
}

steps <- tagList(
  tags$div(
    style = step_style,
    tags$div(style = icon_style("#fee2e2", "#dc2626"), "‚è±Ô∏è"),
    tags$strong(style = "text-align: center;", "Velocidad de entrenamiento")
  ),
  tags$div(
    style = step_style,
    tags$div(style = icon_style("#e0f2fe", "#2563eb"), "üß†"),
    tags$strong(style = "text-align: center;", "Precisi√≥n del modelo")
  ),
  tags$div(
    style = step_style,
    tags$div(style = icon_style("#ecfccb", "#65a30d"), "üíæ"),
    tags$strong(style = "text-align: center;", "Consumo de memoria")
  ),
  tags$div(
    style = step_style,
    tags$div(style = icon_style("#fae8ff", "#a21caf"), "üî•"),
    tags$strong(style = "text-align: center;", "Robustez al overfitting")
  )
)

tags$div(
  style = "
    display: flex;
    justify-content: center;
    flex-wrap: wrap;
    gap: 24px;
    margin-top: 20px;
    font-family: 'Segoe UI', sans-serif;
    font-size: 16px;
  ",
  steps
)

```

LightGBM permite seleccionar entre 4 m√©todos de boosting

| Valor    | ¬øQu√© hace?                                                                   | ¬øCu√°ndo usarlo?                              |
| -------- | ---------------------------------------------------------------------------- | -------------------------------------------- |
| `"gbdt"` | **Gradient Boosted Decision Trees** (por defecto)                            | ‚úîÔ∏è Preciso, funciona bien en casi todo       |
| `"dart"` | Como GBDT, pero **descarta algunos √°rboles al azar**                         | ‚úîÔ∏è Bueno para evitar overfitting             |
| `"goss"` | **Gradient-based One-Side Sampling**: se entrena con un subconjunto de datos | ‚úîÔ∏è M√°s r√°pido, √∫til con muchos datos         |
| `"rf"`   | **Random Forest**: como m√©todo de bagging, no es boosting puro               | üîÅ Solo si necesitas usar √°rboles aleatorios |


### 4. `verbosity`

Controla cu√°nta informaci√≥n imprime LightGBM en consola mientras entrena el modelo.

```{r verbosity, echo=FALSE}
card_style <- "
  display: flex;
  flex-direction: column;
  align-items: center;
  width: 180px;
  padding: 16px;
  border-radius: 16px;
  background: white;
  border: 2px solid #e5e7eb;
  box-shadow: 3px 4px 12px rgba(0,0,0,0.06);
  transition: transform 0.3s ease;
"

icon_style <- function(bg, color = "black") {
  paste0(
    "font-size: 30px; background:", bg,
    "; color:", color,
    "; padding: 10px; border-radius: 50%; margin-bottom: 10px;"
  )
}

verbosity_cards <- tagList(
  tags$div(
    style = card_style,
    tags$div(style = icon_style("#f3f4f6", "#374151"), "üîá"),
    tags$strong("-1"),
    tags$p("No imprime nada. Entrenamiento silencioso.")
  ),
  tags$div(
    style = card_style,
    tags$div(style = icon_style("#fef3c7", "#92400e"), "‚ö†Ô∏è"),
    tags$strong("0"),
    tags$p("Solo muestra errores graves.")
  ),
  tags$div(
    style = card_style,
    tags$div(style = icon_style("#e0f2fe", "#1e40af"), "‚ÑπÔ∏è"),
    tags$strong("1"),
    tags$p("Errores y advertencias (warnings).")
  ),
  tags$div(
    style = card_style,
    tags$div(style = icon_style("#d1fae5", "#047857"), "üîç"),
    tags$strong("2"),
    tags$p("Muestra informaci√≥n del progreso del entrenamiento.")
  ),
  tags$div(
    style = card_style,
    tags$div(style = icon_style("#ede9fe", "#6b21a8"), "üß†"),
    tags$strong("3+"),
    tags$p("Modo detallado. √ötil para debugging profundo.")
  )
)

tags$div(
  style = "
    display: flex;
    justify-content: center;
    flex-wrap: wrap;
    gap: 20px;
    margin-top: 20px;
    font-family: 'Segoe UI', sans-serif;
    font-size: 15px;
  ",
  verbosity_cards
)
```

# Par√°metros de LightGBM (II)

## üå≤ Estructura del √°rbol

### 1. `num_leaves`

Determina cu√°ntas hojas (leaves) puede tener cada √°rbol de decisi√≥n que construye LightGBM. Una hoja es donde termina una decisi√≥n, y contiene la predicci√≥n final para una parte del dataset. Mientras m√°s hojas, m√°s complejo y flexible puede ser el √°rbol.

| Situaci√≥n                    | Valor sugerido      |
| ---------------------------- | ------------------- |
| Dataset peque√±o              | 15‚Äì31 hojas         |
| Dataset grande               | 64‚Äì255 hojas        |
| Muchas variables categ√≥ricas | Menos hojas mejor   |
| Cuando hay overfitting       | Reduce `num_leaves` |

```{r num_leaves, echo=FALSE}
div(
  style = "background: #fff4e5; border-left: 6px solid #f59e0b; padding: 10px; border-radius: 6px; font-size: 22px;",
  HTML("‚ö†Ô∏è <strong>Importante:</strong> <code>num_leaves</code> debe ser <strong>menor que</strong> <code>2^max_depth</code> para evitar <em>overfitting</em>.")
)
```


### 2. `max_depth`

Controla cu√°ntos niveles de decisiones puede tener un √°rbol individual. Si se reduce, se evita que los √°rboles se vuelvan demasiado complejos. Un valor de -1, LightGBM decide por su cuenta (crece hasta donde haga falta).

### 3. `min_data_in_leaf`

Indica la m√≠nima cantidad de observaciones (filas) que debe tener cada hoja en un √°rbol. Una hoja es un grupo final de datos al que el modelo aplica una predicci√≥n. Si una hoja tiene muy pocos datos, puede memorizar el entrenamiento (overfitting). Esto permite controlar la capacidad del √°rbol de especializarse demasiado.

#### ‚öñÔ∏è Recomendaciones pr√°cticas

| Dataset                 | Valor sugerido |
| ----------------------- | -------------- |
| Peque√±o (<10,000 filas) | 10‚Äì50          |
| Mediano                 | 100‚Äì500        |
| Grande                  | 500‚Äì1000 o m√°s |


# Par√°metros de LightGBM (III)

## üê¢ Control del aprendizaje

### 1. `learning_rate`

Define cu√°nto se ajusta el modelo en cada iteraci√≥n (√°rbol). Si el `learning_rate` es muy grande, el modelo aprende r√°pido, pero puede pasarse de largo.

#### üìä Efecto seg√∫n el valor

| `learning_rate`    | Qu√© pasa                                               |
| ------------------ | ------------------------------------------------------ |
| Alto (`‚â• 0.2`)     | Aprende r√°pido, pero puede sobreajustar o divergir     |
| Medio (`0.05‚Äì0.1`) | Equilibrio entre velocidad y precisi√≥n                 |
| Bajo (`< 0.05`)    | Aprende lento pero con mayor control y precisi√≥n final |

```{r learning_rate, echo=FALSE}
div(
  style = paste(
    "background: #fff7ed;",
    "border-left: 6px solid #f97316;",
    "padding: 10px;",
    "border-radius: 6px;",
    "line-height: 1.5;",
    "font-size: 22px;",
    "margin: auto;"
  ),
  HTML("<strong style='color:#c2410c;'>‚ö†Ô∏è Cuidado:</strong><br>"),
  p(HTML("Valores muy altos <code>(learning_rate &gt; 0.3)</code> ‚Üí el modelo puede ser <strong>inestable</strong> y <em>no generalizar bien</em>.")),
  p(HTML("Valores muy bajos <code>(learning_rate &lt; 0.01)</code> ‚Üí el modelo <strong>entrena muy lento</strong> y requiere m√°s <em>fine tuning</em>."))
)
```

### 2. `nrounds`

Define cu√°ntos √°rboles se van a construir durante el entrenamiento. Este par√°metro va fuera de params. Se usa en validaci√≥n con lgb.cv() o lgb.train() si hay conjunto de validaci√≥n.

#### üß™ Recomendaci√≥n pr√°ctica

| `learning_rate` | `nrounds` sugerido |
| --------------- | ------------------ |
| `0.1`           | 100‚Äì500            |
| `0.05`          | 500‚Äì1000           |
| `0.01`          | 1000‚Äì3000          |

```{r nrounds, echo=FALSE}
div(
  style = paste(
    "background: #ecfdf5;",
    "border-left: 6px solid #10b981;",
    "padding: 10px;",
    "border-radius: 6px;",
    "line-height: 1.5;",
    "font-size: 22px;",
    "margin: auto;"
  ),
  HTML("<strong style='color:#047857;'>‚úÖ Tip importante:</strong><br>"),
  p(HTML("üìâ Us√° <strong>validaci√≥n temprana</strong> (<em>early stopping</em>) para evitar entrenar de m√°s.")),
  p(HTML("Esto detiene el entrenamiento cuando el modelo <strong>deja de mejorar</strong> sobre un conjunto de validaci√≥n."))
)
```

# Par√°metros de LightGBM (IV)

## üõë Regularizaci√≥n y generalizaci√≥n

### 1. `early_stopping_round`

Detiene el entrenamiento autom√°ticamente si el modelo no mejora en un n√∫mero determinado de iteraciones consecutivas sobre un conjunto de validaci√≥n.

```{r early_stopping_round, echo=FALSE}
card_style <- "
  display: flex;
  flex-direction: column;
  align-items: center;
  width: 160px;
  background: white;
  border-radius: 12px;
  padding: 16px;
  box-shadow: 2px 2px 8px rgba(0,0,0,0.06);
  transition: transform 0.3s;
  font-size: 22px;
  text-align: center;
"

cards <- tagList(
  tags$div(
    style = card_style,
    tags$span(style = "font-size: 28px;", "‚è±Ô∏è"),
    tags$strong("Ahorra tiempo"),
    "Evita entrenar m√°s rondas de las necesarias."
  ),
  tags$div(
    style = card_style,
    tags$span(style = "font-size: 28px;", "‚ùå"),
    tags$strong("Reduce el sobreajuste"),
    "Se detiene cuando ya no mejora la validaci√≥n."
  ),
  tags$div(
    style = card_style,
    tags$span(style = "font-size: 28px;", "üéØ"),
    tags$strong("Mejora la precisi√≥n"),
    "Ayuda a encontrar el n√∫mero ideal de √°rboles."
  )
)

tags$div(
  style = "display: flex; justify-content: center; gap: 20px; flex-wrap: wrap; margin-top: 20px;",
  cards
)
```

### 2. `bagging_fraction`

Indica el porcentaje de observaciones (filas) usadas para entrenar cada √°rbol. Esto reduce varianza del modelo al hacer que cada √°rbol vea datos distintos. LightGBM utiliza muestreo sin reemplazo en cada √°rbol, de manera de que cada fila solo se utilizara una √∫nica vez en el entrenamiento de cada uno de los arboles de manera aleatoria.

<center>
  <img src="https://www.baeldung.com/wp-content/uploads/sites/4/2023/07/img_64bd9575e6fc8.svg" width="auto" />
</center>

### 3. `feature_fraction`

Indica el porcentaje de columnas (features) seleccionadas aleatoriamente para construir cada √°rbol. Esto introduce variabilidad y reduce la correlaci√≥n entre √°rboles. Es de mucha utilidad cuando hay muchas columnas.

### 4. `min_gain_to_split`

Impide que se creen ramas nuevas si la ganancia no supera cierto umbral, actuando como un filtro para evitar divisiones poco informativas. Esto ayuda a simplificar los √°rboles.

### 5. `lambda_l1`

La regularizaci√≥n L1 (Lasso) castiga la suma de los valores absolutos de las hojas del √°rbol. Esto fuerza la simplicidad empujado pesos de variables poco importantes hacia cero. Suele ser √∫til cuando hay muchas variables poco √∫tiles.

### 6. `lambda_l2`

La regularizaci√≥n L2 (Ridge) penaliza la suma de los cuadrados de los valores en las hojas. Esto evita que las predicciones sean demasiado extremas, ayudandon as√≠ a la estabilidad del modelo.

# Datos para el ejemplo

# Instalaci√≥n

LightGBM est√° disponible en CRAN y puede instalarse con el siguiente c√≥digo R.

```{r instalacion, eval=FALSE}
install.packages("lightgbm", repos = "https://cran.r-project.org")
library(lightgbm)
```

Cada versi√≥n del paquete que ha estado en CRAN tambi√©n est√° disponible en los [LightGBM releases](https://github.com/microsoft/LightGBM/releases), con un formato de nombre lightgbm-{VERSION}-r-cran.tar.gz