---
title: "Introducci√≥n a LightGBM con R"
author: "Carlos A. Torres Cubilla"
output:
  slidy_presentation: 
    number_sections: false
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: false
---

```{r setup, include=FALSE}
library(htmltools)
library(DiagrammeR)
knitr::opts_chunk$set(
  echo = TRUE, 
  message = FALSE, 
  warning = FALSE,
  fig.align='center'
)
```

# ¬øQu√© es LightGBM?

LightGBM es un framework de *gradient boosting* que utiliza algoritmos de aprendizaje basados en √°rboles para crear modelos predictivos de manera eficiente y r√°pida. 

## ¬øQue significa LightGBM?

- üî¶ **Light** = ligero, r√°pido, optimizado
  - Se refiere a que es una versi√≥n optimizada del algoritmo de Gradient Boosting.
  - Est√° dise√±ado para consumir menos memoria y funcionar m√°s r√°pido que alternativas como XGBoost o Random Forests.
- üìà **GBM** = *Gradient Boosting Machine*
  - Un tipo de t√©cnica de ensemble learning que genera muchos modelos d√©biles (generalmente √°rboles de decisi√≥n) de forma secuencial, donde cada modelo aprende de los errores de sus predecesores para crear un modelo fuerte .
  - Se basa en usar el gradiente del error para mejorar paso a paso.

<div style="text-align: center;">
  <img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiN2KPoea9rFZo4nb0SZKrBrEUjNv-xaqB7gF6Htl5lY5AtOmKH1yFalD9Y6XHNNgtUYqsJCPUr-7a4MJIvdcubXogxerrskVqKfQGhKSpUyrnroLhEi6P5vMXqYE22J3_dnLRuWiBv5Nw/s0/Random+Forest+03.gif" style="display: block; margin-left: auto; margin-right: auto;" width="75%"/>
</div>

<div class="gradient-boosting" style="background-color: #f9f9f9; border-left: 5px solid #007ACC; padding: 10px; margin-top: 0px;">
  LightGBM mejora los errores paso a paso combinando √°rboles de decisi√≥n peque√±os.
</div>

## ¬øQui√©n cre√≥ LightGBM?

<div style="display: flex; align-items: center; gap: 1rem;">
  <img src="https://upload.wikimedia.org/wikipedia/commons/9/96/Microsoft_logo_%282012%29.svg" 
       alt="Microsoft Logo" 
       style="height: 50px; margin-bottom: 0;" />

<div>
  <p style="margin: 0;">
    Desarrollado por <strong>Microsoft Research Asia</strong>  
    como parte del toolkit <strong>DMTK</strong> (*Distributed Machine Learning Toolkit*).
  </p>
  <p style="margin: 0;">
    Lanzado en <strong>2016</strong> como proyecto <em>open source</em>.
  </p>
</div>
</div>

> <em>"Creamos LightGBM para resolver problemas del mundo real a gran escala con eficiencia."</em>  
> ‚Äî <strong>Microsoft DMTK Team</strong>

# ¬øC√≥mo funciona el Gradient Boosting?

Podemos resumir el funcionamiento del *Gradient Boosting* en los siguientes 5 pasos: 

```{r boosting_paso_a_paso, echo=FALSE}
step_style <- "
  display: flex;
  flex-direction: column;
  align-items: center;
  width: 160px;
  border: 1px solid #ccc;
  border-radius: 12px;
  padding: 12px;
  box-shadow: 2px 2px 8px rgba(0,0,0,0.1);
  transition: transform 0.3s;
"

steps <- tagList(
  tags$div(
    style = step_style,
    tags$span(style = "font-size: 30px;", "üìÖ"),
    tags$strong("1. Predicci√≥n Inicial"),
    tags$p("Promedio de la variable objetivo o log-odds si es clasificaci√≥n.")
  ),
  tags$div(
    style = step_style,
    tags$span(style = "font-size: 30px;", "üìà"),
    tags$strong("2. C√°lculo de Error"),
    tags$p("Se calcula la diferencia entre la predicci√≥n y el valor real.")
  ),
  tags$div(
    style = step_style,
    tags$span(style = "font-size: 30px;", "üå≥"),
    tags$strong("3. Nuevo √°rbol"),
    tags$p("Se entrena un √°rbol sobre los errores residuales (gradientes).")
  ),
  tags$div(
    style = step_style,
    tags$span(style = "font-size: 30px;", "‚öñÔ∏è"),
    tags$strong("4. Ponderar"),
    tags$p("Se multiplica por el learning rate para suavizar el ajuste.")
  ),
  tags$div(
    style = step_style,
    tags$span(style = "font-size: 30px;", "‚ûï"),
    tags$strong("5. Sumar"),
    tags$p("Se agrega el √°rbol al modelo existente.")
  )
)

tags$div(
  style = "display: flex; justify-content: center; gap: 20px; flex-wrap: wrap; margin-top: 20px;",
  steps
)
```


üîÑ El proceso se repite muchas veces, agregando un √°rbol nuevo en cada paso, hasta que se alcanza el n√∫mero m√°ximo de rondas (nrounds) o se activa un criterio de parada temprana (early_stopping_rounds) si el rendimiento en el conjunto de validaci√≥n deja de mejorar.

<center>
  <img src="https://media.geeksforgeeks.org/wp-content/uploads/20250519125344578128/python.webp" width="800" />
</center>

Estos pasos se pueden expresar de manera matem√°tica de manera facil

### Paso 1: Predicci√≥n Inicial

El modelo comienza con una predicci√≥n constante para todos los datos.

- En regresi√≥n, suele ser: \(F_0(x) = \bar{y} \)

- En clasificaci√≥n binaria, suele ser: \( F_0(x) = \log\left(\frac{p}{1 - p}\right), \quad p = \frac{\text{# de clase 1}}{n} \)

### Paso 2: Calcular el Error o Gradiente

Para cada observaci√≥n, se calcula cu√°nto se est√° equivocando el modelo actual:

\( g_i^{1} = y_i - F_{0}(x_i) \)

Este paso corresponde al c√°lculo del gradiente de la funci√≥n de p√©rdida con respecto a la predicci√≥n actual.

### Paso 3: Entrenar un nuevo modelo sobre ese error

Se entrena un modelo d√©bil que aprenda a corregir esos errores: \( h_1(x) \approx g_i^{1} \)

### Paso 4: Escalar la correcci√≥n con un Learning Rate
Se ajusta la magnitud de la correcci√≥n aplicando un factor de aprendizaje \( \eta \) (por ejemplo, 0.1):

\( \text{correcci√≥n} = \eta \cdot h_1(x) \)

Esto suaviza el aprendizaje y previene el sobreajuste.

### Paso 5: Actualizar el modelo

Se suma la correcci√≥n al modelo acumulado anterior:

\( F_m(x) = F_0(x) + \eta \cdot h_1(x) \)

Este proceso se repite durante varias iteraciones, agregando un nuevo modelo d√©bil \( h(x) \) cada vez, hasta alcanzar un n√∫mero m√°ximo de rondas (*nrounds*) o un criterio de parada temprana (*early_stopping_rounds*)

### Resultado final

La predicci√≥n final del modelo se puede representar mediante la siguiente funci√≥n:

\( F_m(x) = F_0(x) + \sum_{m=1}^M \eta \cdot h_m(x)\)

Donde: 

- \( F_m(x) \): predicci√≥n final,
- \( \eta \): learning rate,
- \( h_m(x) \): modelo d√©bil,
- \( M \): n√∫mero total de iteraciones.

En la siguiente imagen se representa visualmente el proceso de mejora paso a paso de un modelo basado en *Gradien Boosting*. 

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(lightgbm)
library(ggplot2)
library(dplyr)

# Datos simulados
set.seed(123)
n <- 300
x <- runif(n, 0, 10)
y <- sin(x) + rnorm(n, 0, 0.3)
data <- data.frame(x = x, y = y)

# Dataset para LightGBM
X_mat <- matrix(x, ncol = 1)
dtrain <- lgb.Dataset(data = X_mat, label = y)

# Modelo completo con 10 iteraciones
params <- list(
  objective = "regression",
  learning_rate = 0.3,
  num_leaves = 4,
  max_depth = 2,
  verbosity = -1
)
model <- lgb.train(params, dtrain, nrounds = 12)

# Predicciones paso a paso
predictions <- lapply(1:12, function(i) {
  data.frame(
    x = x,
    y_pred = predict(model, X_mat, num_iteration = i),
    paso = paste("Paso", i)
  )
}) %>% bind_rows()

# Convertir a factor con niveles ordenados
predictions$paso <- factor(predictions$paso, levels = paste("Paso", 1:12))

# Visualizaci√≥n
ggplot(predictions, aes(x = x, y = y_pred)) +
  geom_line(color = "blue", alpha = 0.7) +
  geom_point(data = data, aes(x = x, y = y), alpha = 0.2) +
  facet_wrap(~ paso, ncol = 4) +
  theme_minimal(base_size = 12) +
  labs(title = "Gradient Boosting: Mejora paso a paso", x = "x", y = "Predicci√≥n") +
  theme(plot.title = element_text(hjust = 0.5)) 
```

Cada iteraci√≥n mejora el modelo poco a poco, corrigiendo errores anteriores.

# Ventajas y desventajas

LightGBM se destaca no solo por su rendimiento predictivo, sino tambi√©n por su eficiencia computacional. Su adopci√≥n generalizada en aplicaciones reales de machine learning se puede explicar mediante cinco ventajas clave.

## Ventajas

```{r ventajas, echo=FALSE}
ventajas_style <- "
  display: flex; 
  flex-direction: column; 
  align-items: center; 
  width: 120px; 
  transition: transform 0.3s;
"

tags$div(
  style = "display: flex; justify-content: center; gap: 40px; margin-top: 20px;",

  tags$div(
    style = ventajas_style,
    onmouseover = sprintf("this.style.transform='scale(1.1)'"),
    onmouseout = sprintf("this.style.transform='scale(1)'"),
    tags$span(style = "font-size: 40px;", "‚ö°"),
    tags$p("Velocidad")
  ),

  tags$div(
    style = ventajas_style,
    onmouseover = sprintf("this.style.transform='scale(1.1)'"),
    onmouseout = sprintf("this.style.transform='scale(1)'"),
    tags$span(style = "font-size: 40px;", "üíæ"),
    tags$p("Memoria")
  ),

  tags$div(
    style = ventajas_style,
    onmouseover = sprintf("this.style.transform='scale(1.1)'"),
    onmouseout = sprintf("this.style.transform='scale(1)'"),
    tags$span(style = "font-size: 40px;", "üéØ"),
    tags$p("Precisi√≥n")
  ),

  tags$div(
    style = "display: flex; flex-direction: column; align-items: center; width: 120px; transition: transform 0.3s;",
    onmouseover = sprintf("this.style.transform='scale(1.1)'"),
    onmouseout = sprintf("this.style.transform='scale(1)'"),
    tags$span(style = "font-size: 40px;", "üñ•Ô∏è"),
    tags$p("Paralelismo")
  ),

  tags$div(
    style = ventajas_style,
    onmouseover = sprintf("this.style.transform='scale(1.1)'"),
    onmouseout = sprintf("this.style.transform='scale(1)'"),
    tags$span(style = "font-size: 40px;", "üìä"),
    tags$p("Escalabilidad")
  )
)
```

<div style="margin: 0 auto; width: fit-content;">
| Ventaja       | Descripci√≥n                                                                 |
|---------------|------------------------------------------------------------------------------|
| Velocidad   | Entrena modelos r√°pidamente, lo que lo hace ideal para proyectos con grandes vol√∫menes de datos |
| Memoria     | Optimiza el consumo de memoria, permitiendo trabajar con datasets grandes sin requerir tanta RAM         |
| Precisi√≥n   | Proporciona resultados precisos y competitivos en tareas de predicci√≥n                    |
|  Paralelismo | Aprovecha m√∫ltiples n√∫cleos, cl√∫steres y aceleraci√≥n por GPU para entrenamientos m√°s r√°pidos            |
| Escalabilidad | Maneja grandes vol√∫menes de datos sin p√©rdida de rendimiento       |
</div>

## Desventajas

```{r desventajas, echo=FALSE}
desventajas_style <- "
  display: flex; 
  flex-direction: column; 
  align-items: center; 
  width: 130px; 
  transition: transform 0.3s;
"

tags$div(
  style = "display: flex; justify-content: center; flex-wrap: wrap; gap: 40px; margin-top: 20px;",

  tags$div(
    style = desventajas_style,
    onmouseover = "this.style.transform='scale(1.1)'",
    onmouseout = "this.style.transform='scale(1)'",
    tags$span(style = "font-size: 40px;", "‚ö†Ô∏è"),
    tags$p("Overfitting")
  ),

  tags$div(
    style = desventajas_style,
    onmouseover = "this.style.transform='scale(1.1)'",
    onmouseout = "this.style.transform='scale(1)'",
    tags$span(style = "font-size: 40px;", "üîç"),
    tags$p("Dif√≠cil de interpretar")
  ),

  tags$div(
    style = desventajas_style,
    onmouseover = "this.style.transform='scale(1.1)'",
    onmouseout = "this.style.transform='scale(1)'",
    tags$span(style = "font-size: 40px;", "üìâ"),
    tags$p("Problemas en datasets peque√±os")
  ),

  tags$div(
    style = desventajas_style,
    onmouseover = "this.style.transform='scale(1.1)'",
    onmouseout = "this.style.transform='scale(1)'",
    tags$span(style = "font-size: 40px;", "üß†"),
    tags$p("Curva de aprendizaje")
  ),

  tags$div(
    style = desventajas_style,
    onmouseover = "this.style.transform='scale(1.1)'",
    onmouseout = "this.style.transform='scale(1)'",
    tags$span(style = "font-size: 40px;", "üîå"),
    tags$p("Soporte parcial en entornos")
  )
)
```

<div style="margin: 0 auto; width: fit-content;">
| ‚ö†Ô∏è Desventaja                | üìù Descripci√≥n                                                                 |
|-----------------------------|--------------------------------------------------------------------------------|
| Overfitting                 | Al ser muy poderoso, puede memorizar ruido si no se ajusta correctamente.      |
| Dif√≠cil de interpretar      | Es una "caja negra" frente a modelos m√°s simples como regresi√≥n lineal.        |
| Problemas en datasets peque√±os | Modelos simples pueden generalizar mejor en conjuntos de datos chicos.          |
| Curva de aprendizaje        | Muchos hiperpar√°metros que requieren experiencia para ajustar bien.            |
| Soporte parcial en entornos | APIs disponibles, pero algunas integraciones pueden ser limitadas o inestables. |
</div>



# ¬øPara qu√© utilizar LightGBM?

LightGBM es  altamente vers√°til que permite resolver una amplia variedad de problemas, desde tareas tradicionales como clasificaci√≥n y regresi√≥n, hasta aplicaciones m√°s complejas como ranking, detecci√≥n de fraude y sistemas de recomendaci√≥n personalizados. Esto lo convierte en una herramienta poderosa y escalable para proyectos de aprendizaje autom√°tico en entornos reales.

```{r uso, echo=FALSE}
browsable(
  tags$div(
    style = "
      display: flex;
      justify-content: center;
      align-items: center;
      background: transparent;
    ",
    tags$style(HTML("
      /* Reset total de margenes/paddings en Slidy */
      body, html, .slide, .content {
        margin: 0 !important;
        padding: 0 !important;
        height: 0% !important;
      }

      /* El SVG generado por DiagrammeR */
      svg {
        display: block;
        margin: 0;
        padding: 0;
        height: auto;
        width: 0;
        max-height: 30vh;
      }

      .grViz {
        display: flex;
        justify-content: center;
        align-items: center;
        padding: 0 !important;
        margin: 0 !important;
        height: auto;
        width: 0;
        max-height: 30vh;
      }
      }
    ")),
    grViz("
      digraph lightgbm {
        graph [rankdir = TB, margin=0, nodesep=0.3, ranksep=0.3]
        node [shape = box, style = filled, fillcolor = LightGray, fontsize = 12, margin=0.05]

        Usos [label = '¬øPara qu√© se usa?', shape=box, fillcolor=lightblue]
        Clasificacion [label = 'Clasificaci√≥n']
        Regresion [label = 'Regresi√≥n']
        Ranking [label = 'Ranking']
        DeteccionFraude [label = 'Detecci√≥n de\\nfraude']
        Recomendacion [label = 'Sistemas de\\nrecomendaci√≥n']

        Usos -> Clasificacion
        Usos -> Regresion
        Usos -> Ranking
        Usos -> DeteccionFraude
        Usos -> Recomendacion
      }
    ")
  )
)
```

Much√≠simas empresas en todo el mundo utilizan LightGBM debido a su velocidad y precisi√≥n para resolver distintos problemas de machine learning. Algunos ejemplos de compa√±√≠as l√≠deres que han implementado LightGBM en sus soluciones para estos casos de uso:

```{r empresas, echo=FALSE}
html <- tags$div(
  # Estilo del contenedor general
  style = "display: flex; justify-content: center; flex-wrap: wrap; gap: 40px; margin-top: 20px;",
  
  # Estilo para flip cards
  tags$style(HTML("
    .flip-card {
      background-color: transparent;
      width: 175px;
      height: 250px;
      perspective: 1000px;
    }

    .flip-card-inner {
      position: relative;
      width: 100%;
      height: 100%;
      transform-style: preserve-3d;
      transition: transform 0.6s ease;
      transform-origin: center center;
    }

    .flip-card:hover .flip-card-inner {
      transform: rotateY(180deg);
    }

    .flip-card-front, .flip-card-back {
      position: absolute;
      width: 100%;
      height: 100%;
      backface-visibility: hidden;
      border-radius: 10px;
      box-shadow: 0 4px 8px rgba(0,0,0,0.1);
      display: flex;
      flex-direction: column;
      align-items: center;
      justify-content: center;
      padding: 10px;
      box-sizing: border-box;
      text-align: center;
      overflow: hidden;
    }

    .flip-card-front {
      background: white;
    }

    .flip-card-back {
      background-color: #f0f0f0;
      transform: rotateY(180deg);
      font-size: 0.75em;
    }

    .flip-card img {
      width: 90px;
      height: auto;
      margin-bottom: 10px;
    }
    
  ")),

  # Contenido de las tarjetas
  lapply(list(
    list(nombre = "Microsoft", uso = "Clasificaci√≥n", logo = "https://upload.wikimedia.org/wikipedia/commons/9/96/Microsoft_logo_%282012%29.svg", detalle = "Categorizar correos y detectar spam"),
    list(nombre = "Uber", uso = "Regresi√≥n", logo = "https://upload.wikimedia.org/wikipedia/commons/5/58/Uber_logo_2018.svg", detalle = "Estimar demanda y tiempos de llegada"),
    list(nombre = "LinkedIn", uso = "Ranking", logo = "https://upload.wikimedia.org/wikipedia/commons/thumb/a/aa/LinkedIn_2021.svg/2560px-LinkedIn_2021.svg.png", detalle = "Ordenar resultados de b√∫squeda y recomendaciones de empleo"),
    list(nombre = "PayPal", uso = "Detecci√≥n de fraude", logo = "https://upload.wikimedia.org/wikipedia/commons/b/b5/PayPal.svg", detalle = "Detectar transacciones sospechosas en tiempo real"),
    list(nombre = "Netflix", uso = "Sistemas de recomendaci√≥n", logo = "https://upload.wikimedia.org/wikipedia/commons/0/08/Netflix_2015_logo.svg", detalle = "Personalizar sugerencias de contenido")
  ), function(e) {
    tags$div(class = "flip-card",
      tags$div(class = "flip-card-inner",
        tags$div(class = "flip-card-front",
          tags$img(src = e$logo),
          tags$p(e$uso, style = "color: gray; font-size: 0.85em;")
        ),
        tags$div(class = "flip-card-back",
          tags$strong(e$nombre),
          tags$p(e$detalle)
        )
      )
    )
  })
)

browsable(html)
```

# Par√°metros de LightGBM

Cuando entrenamos un modelo con lightgbm, necesitamos decirle c√≥mo aprender. Los par√°metros del modelo le indican a LightGBM como y que tanto aprender. Para una informaci√≥n m√°s completa de los par√°metros de LightGBM visitar la [documentaci√≥n oficial en R](https://lightgbm.readthedocs.io/en/latest/R/index.html).

Los principales par√°metros que debemos conocoer son:

#### 1. objective

Especifica la funci√≥n de p√©rdida que se optimizar√° durante el entrenamiento. Qu√© tipo de problema est√°s resolviendo. Los principales valores de este par√°metro son:

- `"regression"` ‚Üí si es un valor num√©rico continuo

- `"binary"` ‚Üí si es una clasificaci√≥n con dos clases

- `"multiclass"` ‚Üí si hay m√°s de dos clases

#### 2. metric

Es la m√©trica principal que utilizar√° el modelo para medir si el modelo est√° aprendiendo bien.

Ejemplos:

- `"rmse"` ‚Üí error cuadr√°tico medio (regresi√≥n)

- `"binary_logloss"` ‚Üí para clasificaci√≥n binaria

- `"multi_logloss"` ‚Üí clasificaci√≥n m√∫ltiple

#### 3. num_leaves

Determina el tama√±o del √°rbol de decisi√≥n. M√°s hojas significa m√°s complejidad, pero tambi√©n m√°s riesgo de sobreajuste. recomienda empezar con 31 o menos.

#### 4. learning_rate

Controla cu√°nto cambia el modelo en cada iteraci√≥n. Mientras m√°s bajo la convergencia es m√°s lenta, pero m√°s precisa. Se recomienda empezar con 0.1 o incluso 0.05.

#### 5. max_depth

Indica cu√°n profundo pueden ser los √°rboles. Es muy √∫til para limitar la complejidad.

#### 6. early_stopping_round

Detiene el entrenamiento si no mejora en cierto n√∫mero de iteraciones.

#### 7. nrounds

N√∫mero total de iteraciones (√°rboles) que se van a construir.Este par√°metro va fuera de params

Se usa en validaci√≥n con lgb.cv() o lgb.train() si hay conjunto de validaci√≥n.

# Instalaci√≥n

```{r instalacion}
# Instalar solo si no est√°n instalados
# install.packages("lightgbm", repos = "https://cran.r-project.org")
# install.packages("Matrix")
# install.packages("data.table")

library(lightgbm)
library(Matrix)
library(data.table)

```